


\section{Correct rounding and elementary functions}
\label{sect:intro}

The need for accurate elementary functions is important in many
critical programs.  Methods for computing these functions include
table-based methods\cite{Far81,Tan91}, polynomial approximations and
mixed methods\cite{DauMor2k}. See the books by Muller\cite{Muller97} or
Markstein\cite{Markstein2000} for recent surveys on the subject.

The IEEE-754 standard for floating-point arithmetic\cite{IEEE754}
defines the usual floating-point formats (single and double
precision). It also specifies the behavior of the four basic operators
($+,-,\times,\div$) and the square root in four rounding modes (to the
nearest, towards $+\infty$, towards $-\infty$ and towards $0$). Its
adoption and widespread use have increased the numerical quality of,
and confidence in floating-point code. In particular, it has improved
\emph{portability} of such code and allowed construction of
\emph{proofs} on its numerical behavior. Directed rounding modes
(towards $+\infty$, $-\infty$ and $0$) also enabled efficient
\emph{interval arithmetic}\cite{Moore66,KKLRW93}.

However, the IEEE-754 standard specifies nothing about elementary
functions, which limits these advances to code excluding such
functions.  Currently, several options exist: on one hand, one can use
today's mathematical libraries that are efficient but without any
warranty on the correctness of the results. When strict guarantees are
needed, some multiple-precision packages like MPFR \cite{MPFRweb}
offer correct rounding in all rounding modes, but are several orders
of magnitude slower than the usual mathematical libraries for the same
precision. The recently released IBM Ultimate Math
Library\cite{IBMlibultimweb} is both portable and fast, if bulky. It
claims to offer correct rounding to the nearest, however this claim is
not proven, and indeed some values are still uncorrectly
rounded\footnote{Try
  $\sin(0.252113814338773334355892075109295547008514404296875)$}.
Besides, this library still lacks directed rounding modes needed for
interval arithmetic.


The  goal of the \crlibm\ project is to build on a combination of several
recent advances to design a proven  correctly rounded mathematical
library which is fast enough to replace the existing libraries, at acceptable
 cost in terms of performance and resources.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A methodology for efficient correctly-rounded functions}
\label{section:methodology}


\subsection{The Table Maker's Dilemma}

With a few exceptions, the image $\hat{y}$ of a floating-point number $x$ by
a transcendental function $f$ is a transcendental number, and can
therefore not be represented exactly in standard numeration systems.
The only hope is to compute the floating-point number that is closest
to (resp.  immediately above or immediately below) the mathematical
value, which we call the result \emph{correctly rounded} to the
nearest (resp.  towards $+\infty$ or towards $-\infty$).

It is only possible to compute an approximation ${y}$ to the real
number $\hat{y}$ with precision $\maxeps{}$. This ensures that the real value
$\hat{y}$ belongs to the interval $[{y}(1-\maxeps{}) , {y}(1+\maxeps{})]$.
Sometimes however, this information is not enough to decide correct
rounding. For example, if $[{y}(1-\maxeps{}) , {y}(1+\maxeps{})]$
contains the middle of two consecutive floating-point numbers, it is
impossible to decide which of these two numbers is the correctly
rounded to the nearest of $\hat{y}$. This is known as the Table Maker's
Dilemma (TMD).

\subsection{The onion peeling strategy}

A method described by Ziv \cite{Ziv91} is to increase the precision
$\maxeps$ of the approximation until the correctly rounded value can
be decided.  Given a function $f$ and an argument $x$, the value of
$f(x)$ is first evaluated using a quick approximation of precision
$\maxeps_1$.  Knowing $\maxeps_1$, it is possible to decide if
rounding is possible, or if more precision is required, in which case
the computation is restarted using a slower approximation of precision
$\maxeps_2$ greater than $\maxeps_1$, and so on. This approach makes
sense even in terms of average performance, as the slower steps are
rarely taken.

However there was until recently no practical bound on the termination
time of such an algorithm. This iteration has been proven to
terminate, but the actual maximal precision required in the worst case
is unknown.  This might prevent using this method in critical
application.




\section{The Correctly Rounded Mathematical Library}
\label{section:crlibm}

Our own library, called \crlibm\ for \emph{correctly rounded
  mathematical library}, is based on the work of
Lef\`evre and Muller \cite{LMT98,Lef2000} who computed the worst-case $\maxeps$
required for correctly rounding several functions in double-precision
over selected intervals in the four IEEE-754 rounding modes. For
example, they proved that 157 bits are enough to ensure correct rounding
of the exponential function on all of its domain for the four IEEE-754
rounding modes.

\subsection{Two steps are enough}
Thanks to such results, we are able to guarantee correct rounding in
two iterations only, which we may then optimize separately. The first
of these iterations is relatively fast and provides between 60 and 80
bits of accuracy (depending on the function), which is sufficient in
most cases. It will be referred throughout this document as the \quick\ 
phase of the algorithm. The second phase, referred to as the
\accurate\ phase, is dedicated to challenging cases. It is slower but
has a reasonably bounded execution time, tightly targeted at
Lef\`evre's worst cases.

Having a proven worst-case execution time lifts the last obstacle to a
generalization of correctly rounded transcendentals. Besides, having
only two steps allows us to publish, along with each function, a proof
of its correctly rounding behavior.


\subsection{Portable IEEE-754 FP for fast first step}
The computation of a tight bound on the approximation error of the
first step ($\maxeps_1$) is crucial for the efficiency of the onion
peeling strategy: overestimating $\maxeps_1$ means going more often
than needed through the second step. As we want the proof to be
portable as well as the code, our first steps are written in strict
IEEE-754 arithmetic. On some systems, this means preventing the
compiler/processor combination to use advanced floating-point features
such as fused multiply-and-add or extended double precision. It also
means that the performance of our portable library will be lower than
optimized libraries using these features.

To ease these proofs, our first steps make wide use of classical, well
proven results like Sterbenz' lemma. When a result is needed in a
precision higher than double precision (as is the case of $y_1$,
the result of the first step), it is represented as as the sum of two
floating-point numbers, also called a \emph{double-double} number.
There are well-known algorithms for computing on double-doubles, and
they are presented in the next chapter. An advantage of properly
encapsulating double-double arithmetic is that we can actually exploit
fused multiply-and-add operators in a transparent manner (this
experimental feature is currently available for the Itanium and
PowerPC platforms, when using the \texttt{gcc} compiler).

At the end of the \quick\ phase, a sequence of simple tests on
$y_1$ knowing $\maxeps_1$ allows to decide whether to go for
the second step. The sequence corresponding to each rounding mode is
shared by most functions and is also carefully proven in the next
chapter.


\subsection{Software Carry-Save for an accurate second step}
For the second step, we designed an ad-hoc multiple-precision library
called Software Carry-Save library \emph{(scslib)} which is lighter
and faster than other available libraries for this specific
application \cite{DefDin2002,DinDef2003}. This choice is motivated by
considerations of code size and performance, but also by the need to
be independent of other libraries: Again, we need a library on which
we may rely at the proof level. This library is included in \crlibm,
but also distributed separately \cite{SCSweb}.


\subsection{Error analysis and the accuracy/performance tradeoff\label{sec:error-accuracy-perf}}

TODO better...

The error analysis for a function evaluation aims at providing an
overall relative error of the whole evaluation process, which can then
be used to test rounding thanks to theorems given in
\ref{section:testrounding}. Care must be taken that only
\emph{absolute} error terms (noted $\delta$) can be added, although
some error terms are best expressed as \emph{relative} (noted
$\epsilon$), like the the rounding error of an IEEE operation, or the
minimax approximation error. Remark also that the error needed for the
theorems in \ref{section:testrounding} is a \emph{relative} error.
Managing the relative and absolute error terms is very dependent on
the function, and usually involves keeping upper and lower bounds on
the values manipulated along with the error terms. 

Error terms to consider are the following:
\begin{itemize}
\item approximation errors  (minimax or Taylor),
\item rounding error, which fall into two categories:
  \begin{itemize}
  \item roundoff errors in values tabulated as doubles or
    double-doubles (with the exception of roundoff errors on the coefficient
    of a polynomial, which are counted in the appproximation error),
  \item roundoff errors in IEEE-compliant operations.
  \end{itemize}
\end{itemize}




\subsection{Current state of \emph{crlibm}}

The library \texttt{crlibm} \emph{(correctly rounded mathematical
  library)} currently offers accurate parts for the exponential,
logarithm in radix $2$, $10$ and $e$, sine, cosine, tangent,
arctangent, plus trigonometric argument reduction. The quick part and
its proof have been written for the exponential, natural logarithm,
sine, cosine tangent, arctangent, and the hyperbolic sine and cosine
thus far.

The difficulty is to prove both the algorithm and the C program.
The proofs rely heavily on several shared lemmas, assuming the good
behavior of the system composed of the compiler and the processor.
Another difficulty is that performance is important.



\section{An overview of other  available mathematical libraries\label{section:lib-overview}}

Many high-quality mathematical libraries are freely available and have
been a source of inspiration for this work.

Most mathematical libraries do not offer correct rounding. They can be classified as 
\begin{itemize}
\item portable libraries  assuming IEEE-754
  arithmetic, like \emph{fdlibm}, written by Sun\cite{FDLIBMweb};
\item  Processor-specific libraries, by
  Intel\cite{HarKubStoTan99,IntelOpenSource} and
  HP\cite{Markstein2000,Markstein2001} among other.
\end{itemize}

Operating systems often include several mathematical libraries, some of which are derivatives of one
of the previous.

Two libraries offer correct correct rounding:
\begin{itemize}
\item The \emph{libultim} library, also called MathLib, is developed at
  IBM by Ziv and others \cite{IBMlibultimweb}. It provides correct rounding,
  under the assumption that 800 bits are enough in all case. This
  approach suffers two weaknesses. The first is the absence of proof
  that 800 bits are enough: all there is is a very high probability.
  The second is that, as we will see in the sequel, for challenging
  cases, 800 bits are much of an overkill, which can increase the
  execution time up to 20,000 times a normal execution. This will
  prevent such a library from being used in real-time applications.
  Besides, to prevent this worst case from degrading average
  performance, there is usually some intermediate levels of precision
  in MathLib's elementary functions, which makes the code larger, more
  complex, and more difficult to prove.
  
  In addition this library provides correct rounding only to nearest.
  This is the most used rounding mode, but it might not be the most
  important as far as correct rounding is concerned: correct rounding
  provides a precision improvement over current mathematical libraries
  of only a fraction of a {unit in the last place} \emph{(ulp)}.
  Conversely, the three other rounding modes are needed to guarantee
  intervals in interval arithmetic.  Without correct rounding in these
  directed rounding modes, interval arithmetic looses up to one
  \emph{ulp} of precision in each computation.
  
\item \emph{MPFR} is a multiprecision package safer than
  \emph{libultilm} as it uses arbitrary multiprecision. It provides
  most of elementary functions for the four rounding modes defined by
  the IEEE-754 standard. However this library is not optimized for
  double precision arithmetic. In addition, as its exponent range is
  much wider than that of IEEE-754, the subtleties of denormal numbers
  are difficult to handle properly using such a multiprecision
  package.
\end{itemize}



\section{Organization of the source code of the library}

For each function, the file containing the source code for the
accurate phase is named after the function itself (for instance
\texttt{exp.c}, \texttt{log.c}), and the quick phase, when available,
is named with the \texttt{\_fast} suffix (for instance
\texttt{exp\_fast.c}). The names of auxiliary files \texttt{.c} or
\texttt{.h} files relative to a function are also prefixed with the
name of the function.

The accurate phase relies on \texttt{scslib}, the \emph{software
  carry-save} multiple-precision library written for this purpose.
This library is contained in a subdirectory called \texttt{scs\_lib}.

The common C routines that are detailed in Chapter~\ref{chap:common} of
this document are defined in \texttt{crlibm\_private.c} and
\texttt{crlibm\_private.h}.

Many of the constants used in the C code have been computed thanks to
Maple procedures which are contained in the \texttt{maple}
subdirectory. Some of these procedures are explained in
Chapter~\ref{chap:common}. For some functions, a Maple procedure
mimicking the C code, and used for debugging or optimization purpose,
is also available.


The code also includes programs to test the \texttt{crlibm} functions
against MPFR and \texttt{libultim}, in terms of correctness and
performance. They are located in the \texttt{tests} directory.

