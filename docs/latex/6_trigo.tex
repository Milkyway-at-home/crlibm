This chapter is contributed by C. Daramy-Loirat, D. Defour
and F. de~Dinechin.  

\section*{Introduction}
This chapter describes the implementations of sine, cosine and
tangent, as they share much of their code. The proof sketch below is
supported by the Maple script \texttt{maple/trigo.mpl} of the \crlibm\
distribution, which implements the computations of error bounds and
validity bounds for the various algorithmic paths described here.

\section{Overview of the algorithms}

\subsection{Exceptional cases}

The three trigonometric functions return NaN for infinite and NaN
arguments, and are defined otherwise. $\tan$ may not return
infinities: an argument based on continued fractions to find the worst
cases for range reduction may also be used to show that the sine of a
floating-point number is always larger than $2^{-150}$, and therefore
never flushes to zero (see \cite{Muller97} p. 151 and following).

For very small arguments,
\begin{itemize}
\item $\sin(x) = x-x^3/6 + O(x^5) = x(1-x^2/6) + O(x^5)$ where
  $O(x^5)$ has the sign of $x$. Therefore $\sin(x)$ is rounded to $x$
  for all the rounding  modes if $|x|<2^{-26}$.
\item $\cos(x) = 1-x^2/2 + O(x^4)$ where $O(x^4)$ is positive.
  Therefore $\cos(x)$ is rounded to $1$ in RN and RU mode if
  $x<\sqrt{2^{-53}}$. In RD and RZ modes, we have $\cos(0)=1$ and
  $\cos(x)=1-2^{-53}$ for $|x|<2^{-26}$.
\item $\tan(x) = x+x^3/3 + O(x^5) = x(1+x^2/3) + O(x^5)$ where
  $O(x^5)$ has the sign of $x$. Therefore $\tan(x)$ is rounded to $x$
  for all the rounding  modes if $|x|<2^{-27}$.
\end{itemize}


\subsection{Argument reduction}

Most implementations of the trigonometric functions have two steps of argument reduction: 
\begin{itemize}
\item first the input number $x$ is reduced to $y\in
  [0;\frac{\pi}{2}]$, with reconstruction using periodicity and
  symmetry properties,
\item then the reduced argument is further broken down as $y=a+z$,
  with reconstruction using the formula for $\sin(a+z)$ and
  $\cos(a+z)$, using tabulated values of $\sin(a)$ and
  $\cos(a)$
\end{itemize}

We chose to implement argument reduction in one step only, which
computes an integer $k$ and a reduced argument y such that

\begin{equation}
  x = k\frac{\pi}{256} + y\label{eq:trigoargred}
\end{equation}
where $k$ is an integer and  $ |y| \leq {\pi}/{512}$.
This step computes $y$ as a double-double: $y\approx y_h+y_l$. 

In the following we note $a=k\pi/256$. 

Then we read off a table 

$$sa_h+sa_l \approx sin(a)$$
$$ca_h+ca_l \approx cos(a)$$

Only 64 quadruples $(sa_h,sa_l,ca_h,ca_l)$ are tabulated (amounting to
$64\times 8 \times 4 = 2048$ bytes), the rest is obtained by
periodicity and symmetry, implemented as masks and integer operations
on the integer $k$. For instance,  $a \mod 2\pi$ is implemented by $k \mod 512$,
$\pi/2-a$ is implemented as $128-k$, etc.



Then we use the reconstruction steps:

\begin{equation}        
  \sin(x) = \sin(a + y) =  \cos(a) \sin(y) +  \sin(a) \cos(y) 
  \label{eq:sinapy}
\end{equation}

\begin{equation}
  \cos(x) = \cos(a + y) = \cos(a) \cos(y) -  \sin(a) \sin(y) 
  \label{eq:cosapy}
\end{equation}

\begin{equation} 
  tan(x) = \frac{\sin(x)}{\cos(x)} = \frac{\cos(a) \sin(y) +  \sin(a) \cos(y)}{\cos(a) \cos(y) -  \sin(a) \sin(y)}
  \label{eq:tanapy}
\end{equation}


\subsection{Polynomial evaluation}


To implement the previous equations, $\cos(y)$ and $\sin(y)$ are
computed as unevaluated $1+t_c$ and $(y_h+y_l)(1+t_s)$ respectively,
where $t_c$ and $t_s$ are doubles computed using a polynomial
approximation of small degree:

\begin{itemize}
\item $t_s = y^2(s_3 + y^2(s_5 + y^2s_7)))$ with $s3$, $s5$ and
$s7$ the Taylor coefficients.
\item $t_c = y^2(c_2 + y^2(c_4 + y^2c_6))$ with $c2$, $c4$ and $c6$ the
Taylor coefficients (or a more accurate minimax approximation).
\end{itemize}



\subsection{Reconstruction}

\subsubsection{Sine}
According to equation (\ref{eq:sinapy}), we have to compute: 
 \begin{eqnarray*}
  \sin(a+y) &=& \sin(a) \cos(y)  + \cos(a)\sin(y)  \\
  & \approx& (sa_h+sa_l)(1+t_c) + (ca_h+ca_l)(y_h+y_l)(1+t_s)
\end{eqnarray*}


Figure~\ref{fig:sine-reconstruction} shows the worst-case respective
orders of magnitude of the terms of this sum. The terms completely to the
right of the $\epsilon$ bar will be neglected, and a bound on the
error thus entailed is computed in the following. Note that the term
$ca_hy_h$ has to be computed exactly by a Mul12.

Finally the reconstruction consists of adding together the lower-order
terms in increasing order of magnitude, and computing the
double-double result by an Add12.

\begin{figure}[htbp]    
  \begin{center}
    \small
    \setlength{\unitlength}{3ex}
      \framebox{
        \begin{picture}(22,9)(-3,-4.2)
          \put(9.5,4){\line(0,-1){8}}  \put(9,4){$\epsilon$}
          
          \put(4,3.2){$sa_h$} \put(0.05,3){\framebox(7.9,0.7){}}
          \put(12,3.2){$sa_l$}  \put(8.05,3){\framebox(7.9,0.7){}}
          
          \put(6,2.2){$sa_ht_c$} \put(2.05,2){\framebox(7.9,0.7){}}
          \put(14,2.2){$sa_lt_c$}  \put(10.05,2){\framebox(7.9,0.7){}}

          \put(4.5,1.2){$ca_hy_h$} \put(0.55,1){\framebox(7.9,0.7){}}
          \put(12.5,1.2){$ca_hy_h$}  \put(8.55,1){\framebox(7.9,0.7){}}

          \put(11.5,0.2){$ca_hy_l $}  \put(7.55,0){\framebox(7.9,0.7){}}
          \put(11.5,-0.8){$ca_ly_h $}  \put(7.55,-1){\framebox(7.9,0.7){}}

         \put(6.5,-1.8){$ca_hy_ht_s$} \put(2.55,-2){\framebox(7.9,0.7){}}
         \put(13.5,-2.8){$ca_hy_lt_s $}  \put(9.55,-3){\framebox(7.9,0.7){}}
         \put(13.5,-3.8){$ca_ly_ht_s $}  \put(9.55,-4){\framebox(7.9,0.7){}}
        
       \end{picture}
     }
   \end{center}
   \caption{The sine reconstruction}
   \label{fig:sine-reconstruction}
 \end{figure}
 
 

\subsubsection{Cosine}
According to equation (\ref{eq:cosapy}), we have to compute in double-double precision:
 \begin{eqnarray*}
  \cos(a+y) &=& \cos(a) \cos(y)  - \sin(a)\sin(y)  \\
  & \approx& (ca_h+ca_l)(1+t_c) - (sa_h+sa_l)(y_h+y_l)(1+t_s)
\end{eqnarray*}

This is similar to the case of the sine, and the respective orders of
magnitude are given by Figure~\ref{fig:sine-reconstruction}.

\begin{figure}[htbp]
  \begin{center}
    \small \setlength{\unitlength}{3ex} \framebox{
      \begin{picture}(22,9)(-3,-4.2)
        \put(9.5,4){\line(0,-1){8}}
        \put(9,4){$\epsilon$}
  
        \put(4,3.2){$ca_h$} \put(0.05,3){\framebox(7.9,0.7){}}
        \put(12,3.2){$ca_l$}  \put(8.05,3){\framebox(7.9,0.7){}}

        \put(6,2.2){$ca_ht_c$} \put(2.05,2){\framebox(7.9,0.7){}}
        \put(14,2.2){$ca_lt_c$}  \put(10.05,2){\framebox(7.9,0.7){}}

        \put(4.5,1.2){$-sa_hy_h$} \put(0.55,1){\framebox(7.9,0.7){}}
        \put(12.5,1.2){$-sa_hy_h$}  \put(8.55,1){\framebox(7.9,0.7){}}

        \put(11.5,0.2){$-sa_hy_l $}  \put(7.55,0){\framebox(7.9,0.7){}}
        \put(11.5,-0.8){$-sa_ly_h $}  \put(7.55,-1){\framebox(7.9,0.7){}}

        \put(6.5,-1.8){$-sa_hy_ht_s$} \put(2.55,-2){\framebox(7.9,0.7){}}
        \put(13.5,-2.8){$-sa_hy_lt_s $}  \put(9.55,-3){\framebox(7.9,0.7){}}
        \put(13.5,-3.8){$-sa_ly_ht_s $}  \put(9.55,-4){\framebox(7.9,0.7){}}
 
        \end{picture}
      }
    \end{center}\centering
    
    \caption{The cosine reconstruction}
    \label{fig:cosine-reconstruction}
  \end{figure}


\subsubsection{Tangent}

The tangent is obtained by the division of the sine by the cosine,
using the \texttt{Div22} procedure which is accurate to  $2^{-104}$.


\subsection{Precision of this scheme}

As we have $|y|<\pi/512<2^{-7}$, this scheme computes these functions
accurately to roughly $53+12$ bits, so these first steps are very
accurate. 
 

\subsection{Organisation of the code}


The code for argument reduction is shared by the three
trigonometric functions. It is detailed and proven in
Section~\ref{trigo:argred}.

Then there are four procedures, respectively called \verb!do_sin_k_zero!,
\verb!do_sin_k_notzero!, \verb!do_cos_k_zero!, \verb!do_cos_k_notzero!
which do the actual computation after argument reduction. The tangent
function is computed by dividing \verb!do_sin! by \verb!do_cos!. These
procedures are studied in section \ref{trigo:auxilliary}.

Finally, each of the three trigonometric functions come in four variants for the four
rounding modes. These four variants differ in the beginning (special
cases) and in the end (rounding), but share the bulk of the
computation. The shared computation is  called
\verb!compute_trig_with_argred!, etc.




\section{Details of argument reduction
\label{trigo:argred}}


We have 4 possible range reductions, depending on the magnitude of the input number:

\begin{itemize}
\item Cody and Waite with 2 constants (the fastest),
\item Cody and Waite with 3 constants (almost as fast),
\item Cody and Waite with 3 constants in double-double and $k$ a
  64-bit int, and 
\item Payne and Hanek, implemented in SCS (the slowest).
\end{itemize}
Each of these range reductions except Payne and Hanek is valid for $x$
smaller than some bound. The computation of these bounds is detailed
below.

Such a range reduction may cancel up to 62 bits according to a program
by Kahan/Douglas available in Muller's book \cite{Muller97} and
implemented as function \texttt{WorstCaseForAdditiveRangeReduction} in
\texttt{maple/common-procedures.mpl}.  However this is not a concern
unless x is close to a multiple of $\pi/2$ (that is, $k \mod 128=0$): in
the general case the reconstruction will add some tabulated non-zero
value, so the error to consider in the range reduction is the absolute
error.  Only in the cases when $k \mod 128=0$ do we need to have 62
extra bits to compute with. This is ensured by using a slower, more
accurate range reduction. As a compensation, in this case when $k \mod
128=0$, there is no table to read and no reconstruction to perform: a
simple polynomial approximation to the function suffices. 

Section \ref{trigo:structargred} details the organization of this
multi-level argument reduction, and is followed by a detailed proof of
each level.


In the following we note $$C=\frac{\pi}{256}$$


\subsection{Structure of the argument reduction
  \label{trigo:structargred}}
The code for the sine is detailed below (from function \verb!compute_sine_with_argred!).

\begin{lstlisting}[caption={Multilevel argument reduction},firstnumber=1]
static void compute_trig_with_argred(double* prh, double* prl,  double x, int absxhi, int function){ 
  double sah,sal,cah,cal, yh, yl, ts,tc, kd; 
  double kch_h,kch_l, kcm_h,kcm_l,  th, tl,sh,sl,ch,cl;
  int k, quadrant, index;
  long long int kl;

  if  (absxhi < XMAX_CODY_WAITE_3) {
    /* Compute k, deduce the table index and the quadrant */
    DOUBLE2INT(k, x * INV_PIO256);
    kd = (double) k;
    quadrant = (k>>7)&3;      
    index=(k&127)<<2;
    if((index == 0)) { 
      /* Here we risk a large cancellation on yh+yl: use double-double RR */
      /* all this is exact */
      Mul12(&kch_h, &kch_l,   kd, RR_DD_MCH);
      Mul12(&kcm_h, &kcm_l,   kd, RR_DD_MCM);
      Add12 (th,tl,  kch_l, kcm_h) ;
      /* only rounding error in the last multiplication and addition */ 
      Add22 (&yh, &yl,    (x + kch_h) , (kcm_l - kd*RR_DD_CL),   th, tl) ;
      goto trigzero;
    } 
    else {      
      /* index <> 0, don't worry about cancellations on yh+yl */
      if (absxhi < XMAX_CODY_WAITE_2) {
	/* CW 2: all this is exact but the rightmost multiplication */
	Add12 (yh,yl,  (x - kd*RR_CW2_CH),  (kd*RR_CW2_MCL) ) ; 
      }
      else { 
	/* CW 3: all this is exact but the rightmost multiplication */
	Add12Cond(yh,yl,  (x - kd*RR_CW3_CH) -  kd*RR_CW3_CM,   kd*RR_CW3_MCL);
      }
    }
    goto trignotzero;
  }

  else if ( absxhi < XMAX_DDRR ) {
    /* x sufficiently small for a Cody and Waite in double-double */
    DOUBLE2LONGINT(kl, x*INV_PIO256);
    kd=(double)kl;
    quadrant = (kl>>7)&3;
    index=(kl&127)<<2;
    if(index == 0) { /* In this case cancellation may occur, so we
			  do the accurate range reduction */
      scs_range_reduction(); 
      /* Now it may happen that the new k differs by 1 of kl, so check that */
      if(index==0) {  /* no surprise */
	goto trigzero;      }
      else { /*recompute index and quadrant */
	if (index==4) kl++;   else kl--; /* no more than one bit difference */
	kd=(double)kl;
	quadrant = (kl>>7)&3;
      }
    }
    /* arrive here if index<>0 */
    /* double-double argument reduction*/
    /* all this is exact */
    Mul12(&kch_h, &kch_l,   kd, RR_DD_MCH);
    Mul12(&kcm_h, &kcm_l,   kd, RR_DD_MCM);
    Add12 (th,tl,  kch_l, kcm_h) ;
    /* only rounding error in the last multiplication and addition */ 
    Add22 (&yh, &yl,    (x + kch_h) , (kcm_l - kd*RR_DD_CL),   th, tl) ;
    goto trignotzero;
  }
  else {
    /* Worst case : x very large, sin(x) probably meaningless, we return
       correct rounding but do't mind taking time for it */
    scs_range_reduction(); 
    quadrant = (k>>7)&3;                                       
    if(index == 0)
      goto trigzero;
    else 
      goto trignotzero;
  }

 trigzero:
  switch(function) {
  case SIN: goto sinzero;
  case COS: goto coszero;
  case TAN: goto tanzero;
  }
 trignotzero:
  switch(function) {
  case SIN: goto sinnotzero;
  case COS: goto cosnotzero;
  case TAN: goto tannotzero;
  }
  
 sinzero:
  if (quadrant&1)
    do_cos_k_zero(prh, prl, yh,yl);
  else 
    do_sin_k_zero(prh, prl, yh,yl);
  if ((quadrant==2)||(quadrant==3)) {
    *prl=-*prl; *prh=-*prh;
  }
  return;

 sinnotzero:
    LOAD_TABLE_SINCOS();
    if (quadrant&1)   
      do_cos_k_notzero(prh, prl, yh,yl,sah,sal,cah,cal);
    else 
      do_sin_k_notzero(prh, prl, yh,yl,sah,sal,cah,cal);
    if ((quadrant==2)||(quadrant==3)) {
      *prl=-*prl; *prh=-*prh;
    }
    return;
 coszero:
  if (quadrant&1)
    do_sin_k_zero(prh, prl, yh,yl);
  else 
    do_cos_k_zero(prh, prl, yh,yl);
  if ((quadrant==1)||(quadrant==2)) {
    *prl=-*prl; *prh=-*prh;
  }
  return;
 cosnotzero:
    LOAD_TABLE_SINCOS();
    if (quadrant&1)   
      do_sin_k_notzero(prh, prl, yh,yl,sah,sal,cah,cal);
    else 
      do_cos_k_notzero(prh, prl, yh,yl,sah,sal,cah,cal);
    if ((quadrant==1)||(quadrant==2)) {
      *prl=-*prl; *prh=-*prh;
    }
    return;

 tanzero:
  if (quadrant&1) {
    do_sin_k_zero(&ch, &cl, yh,yl);
    do_cos_k_zero(&sh, &sl, yh,yl);
    sh=-sh; sl=-sl;
  } else {
    do_sin_k_zero(&sh, &sl, yh,yl);
    do_cos_k_zero(&ch, &cl, yh,yl);
  }
  Div22(prh, prl, sh, sl, ch, cl);
  return;

 tannotzero:
  LOAD_TABLE_SINCOS();
  if (quadrant&1) {
    do_sin_k_notzero(&ch, &cl, yh,yl,sah,sal,cah,cal);
    do_cos_k_notzero(&sh, &sl, yh,yl,sah,sal,cah,cal);
    sh=-sh; sl=-sl;
  } else {
    do_sin_k_notzero(&sh, &sl, yh,yl,sah,sal,cah,cal);
    do_cos_k_notzero(&ch, &cl, yh,yl,sah,sal,cah,cal);
  }
  Div22(prh, prl, sh, sl, ch, cl);
  return;
}

\end{lstlisting}
 

There is one subtlety at lines 46 and following. There we take the
decision of computing a more accurate range reduction depending on the
value of $k\mod 256$. However, in the case when
$x\times\frac{256}{\pi}$ is very close to the middle between two
integers, it may happen (very rarely) that the value of $k\mod 256$
computed by this second range reduction is equal to $\pm 1$. In such
cases, both values work, but we have to design the algorithm
accordingly.

\subsection{Cody and Waite argument reduction with two constants}

Here we split $C$ into two floating-point constants $C_h$ and $C_l$
such that $C_h$ holds 21 bits of the mantissa of $C$ (the rest being
zeroes), and $C_l=\round(C-C_h)$.  The following is an excerpt of the
Maple code that computes these constants, and then computes the
bound on which this argument reduction is valid.

\begin{lstlisting}[caption={Computing constants for Cody and Waite 2},
  firstnumber=1,  language={sh}]% of course it's maple
%Skip a line here, I don't know why, otherwise latex eats the first line

bitsCh_0:=32:  # ensures at least 53+11 bits

# 1/2 <= C/2^(expC+1) <1
Ch:= round(evalf(  C * 2^(bitsCh_0-expC-1))) / (2^(bitsCh_0-expC-1)):
# recompute bitsCh in case we are lucky (and we are for bitsCh_0=32)
bitsCh:=1+log2(op(2,ieeedouble(Ch)[3])) :  # this means the log of the denominator

Cl:=nearest(C - Ch):
# Cody and Waite argument reduction will work for |k|<kmax_cw2
kmax_cw2:=2^(53-bitsCh):

# The constants to move to the .h file
RR_CW2_CH := Ch:
RR_CW2_MCL := -Cl:
XMAX_CODY_WAITE_2 := nearest(kmax_cw2*C):
\end{lstlisting}

The C code that performs the reduction in this case is the following:

\begin{lstlisting}[caption={Cody and Waite argument reduction with two
    constants},firstnumber=1]
      if (absxhi < XMAX_CODY_WAITE_2) { 
	Add12 (yh,yl,  (x - kd*RR_CW2_CH),  (kd*RR_CW2_MCL) ) ;
      }
\end{lstlisting}

Here only the rightmost multiplication involving a rounding: 
\begin{itemize}
\item The multiplication $\mathit{kd}\otimes \mathit{RR\_CW2\_CH}$ is
  exact because $kd$ is a small integer and $\mathit{RR\_CW2\_CH}$
  has enough zeroes in the mantissa.
\item The subtraction is exact thanks to Sterbenz Lemma.
\item The \texttt{Add12} procedure is exact.
\end{itemize}

The following Maple code thus computes the maximum absolute error on
the reduced argument (with respect to the ideal reduced argument) in
this case.
\begin{lstlisting}[caption={Computing constants for Cody and Waite 2},
  firstnumber=1,  language={sh}]% of course it's maple

delta_repr_C_cw2   := abs(C-Ch-Cl);
delta_round_cw2    := kmax_cw2* 1/2 * ulp(Cl) ;
delta_cody_waite_2 := kmax_cw2 * delta_repr_C_cw2 + delta_round_cw2;
# This is the delta on y, the reduced argument
\end{lstlisting}


\subsection{Cody and Waite argument reduction with three constants}
The C code that performs the reduction in this case is the following:

\begin{lstlisting}[caption={Cody and Waite argument reduction with three 
    constants},firstnumber=1]
      else 
	Add12Cond(yh,yl,  (x - kd*RR_CW3_CH) -  kd*RR_CW3_CM,   kd*RR_CW3_MCL);
    }
\end{lstlisting}

Here again all the operations are exact except the rightmost multiplication. 

The following Maple code computes the constants, the bound on $x$ for
this reduction to work, and the resulting absolute error of the reduced
argument with respect to the ideal reduced argument.

\begin{lstlisting}[caption={Computing constants for Cody and Waite 3},
  firstnumber=1,  language={sh}]% of course it's maple
%Skip a line here, I don't know why, otherwise latex eats the first line

bitsCh_0:=21:
Ch:= round(evalf(  C * 2^(bitsCh_0-expC-1))) / (2^(bitsCh_0-expC-1)):
# recompute bitsCh in case we are lucky
bitsCh:=1+log2(op(2,ieeedouble(Ch)[3])) :  # this means the log of the denominator

r := C-Ch:
Cmed := round(evalf(  r * 2^(2*bitsCh-expC-1))) / (2^(2*bitsCh-expC-1)):
bitsCmed:=1+log2(op(2,ieeedouble(Cmed)[3])) :

Cl:=nearest(C - Ch - Cmed):

kmax_cw3 := 2^31:# Otherwise we have integer overflow

# The constants to move to the .h file
RR_CW3_CH  := Ch;
RR_CW3_CM  := Cmed:
RR_CW3_MCL := -Cl:
XMAX_CODY_WAITE_3 := nearest(kmax_cw3*C):

# The error in this case (we need absolute error)
delta_repr_C_cw3   := abs(C - Ch - Cmed - Cl):
delta_round_cw3    := kmax_cw3 * 1/2 * ulp(Cl) :
delta_cody_waite_3 := kmax_cw3 * delta_repr_C_cw3 + delta_round_cw3:
# This is the delta on y, the reduced argument
\end{lstlisting}

\subsection{Cody and Waite argument reduction in double-double}
The C code that performs the reduction in this case is the following:

\begin{lstlisting}[caption={Cody and Waite argument reduction in
    double-double},firstnumber=1]
  if ( absxhi < XMAX_DDRR ) {
    DOUBLE2LONGINT(kl, x*INV_PIO256);
    kd=(double)kl;
    quadrant = (kl>>7)&3;
    index=(kl&127)<<2;
    if(index == 0) { /* In this case cancellation may occur, so we
			  do the accurate range reduction */
      scs_range_reduction(); 
      /* Now it may happen that the new k differs by 1 of kl, so check that */
      if(index==0) {  /* no surprise */
	*pquadrant=quadrant;
	if (quadrant&1)
	  do_cos_k_zero(psh, psl, yh,yl);
	else 
	  do_sin_k_zero(psh, psl, yh,yl);
      	return;
      }
      else { /*recompute index and quadrant */
	if (index==4) kl++;   else kl--; /* no more than one bit difference */
	kd=(double)kl;
	quadrant = (kl>>7)&3;
      }
    }
    /* arrive here if index<>0 */
    *pquadrant=quadrant;
    LOAD_TABLE_SINCOS(); /* in advance */
    /* all this is exact */
    Mul12(&kch_h, &kch_l,   kd, RR_DD_MCH);
    Mul12(&kcm_h, &kcm_l,   kd, RR_DD_MCM);
    Add12 (th,tl,  kch_l, kcm_h) ;
    /* only rounding error in the last multiplication and addition */ 
    Add22 (&yh, &yl,    (x + kch_h) , (kcm_l - kd*RR_DD_CL),   th, tl) ;
    if (quadrant&1)
      do_cos_k_notzero(psh, psl, yh,yl,sah,sal,cah,cal);
    else 
      do_sin_k_notzero(psh, psl, yh,yl,sah,sal,cah,cal);
    return;
\end{lstlisting}


The error and the bound are computed by the following Maple code.

\begin{lstlisting}[caption={Computing constants for Cody and Waite
    double-double}, firstnumber=1,
  language={sh}]% of course it's maple
%Skip a line here, I don't know why, otherwise latex eats the first line

# max int value that we can be produced by DOUBLE2LONGINT
kmax:=2^51-1:
XMAX_DDRR:=nearest(kmax*C);

#in this case we have C stored as 3 doubles
Ch   := nearest(C):
Cmed := nearest(C-Ch):
Cl   := nearest(C-Ch-Cmed):

RR_DD_MCH := -Ch:
RR_DD_MCM := -Cmed:
RR_DD_CL  := Cl:

delta_repr_C := abs(C - Ch - Cmed - Cl):

# and we have only exact Add12 and Mul12  operations. The only place
# with possible rounding errors is:
#       Add22 (pyh, pyl,    (x + kch_h) , (kcm_l - kd*RR_DD_CL),   th, tl) ;
# where (x + kch_h) is exact (Sterbenz) with up to kmax bits of cancellation
# and the error is simply the error in  (kcm_l - kd*RR_DD_CL)
# At the very worst :
delta_round :=
              kmax * 1/2 * ulp(Cl) # for   kd*RR_DD_CL
              + kmax*ulp(Cl)         # for the subtraction
              + 2^(-100) * Pi/512 :    # for the Add22
delta_RR_DD :=  kmax * delta_repr_C + delta_round:

#  the last case, Payne and Hanek reduction, gives a very small delta:
#  red arg is on 9*30 bits, then rounded to a double-double (106 bits)
# This should, of course, be optimized some day
delta_PayneHanek := 2^(-100):

# Finally the max delta on the reduced argument is
delta_ArgRed := max(delta_cody_waite_2, delta_cody_waite_3,
                    delta_RR_DD, delta_PayneHanek):
\end{lstlisting}


\subsection{Payne and Hanek argument reduction }

This argument reduction is very classical (see K.C. Ng's paper) and
the code both too long and too simple to appear here. The Payne and
Hanek reduction uses SCS computations which ensure relative accuracy
of $2^{-200}$. The result is then converted to a double-double.
Even counting a worst-case cancellation of less than 70 bits, the final
absolute error is much smaller than for the other reductions. 

\begin{lstlisting}[caption={Computing constants for Cody and Waite
    double-double}, firstnumber=1,
  language={sh}]% of course it's maple
%Skip a line here, I don't know why, otherwise latex eats the first line

delta_PayneHanek := 2^(-100):
\end{lstlisting}


\subsection{Maximum error of argument reduction }
The error and the bound are computed by the following Maple code.

\begin{lstlisting}[caption={Payne and Hanek error bound}, firstnumber=1,
  language={sh}]% of course it's maple
%Skip a line here, I don't know why, otherwise latex eats the first line

delta_ArgRed := max(delta_cody_waite_2, delta_cody_waite_3,
                    delta_RR_DD, delta_PayneHanek):
\end{lstlisting}

We find that $\maxdelta_{\mathrm{argred}}\approx 2^{-71}$

We will use this absolute value if $k\mod 256\ne 0$, as there will be
an addition of the reduced argument. Otherwise we need to compute the relative error.



\section{Details of auxilliary functions
  \label{trigo:auxilliary}}

\subsection{Errors due to argument reduction}
Upon entering one of \texttt{do\_sin\_k\_zero}, \texttt{do\_sin\_k\_notzero},
\texttt{do\_cos\_k\_zero} or \texttt{do\_cos\_k\_notzero}, we have in
$y_h+y_l$ an approximation to the ideal reduced value
$\hat{y}=x-k\pi/256$ with a relative accuracy $\epsilon_{\mathrm{argred}}$:

\begin{equation}
  y_h+y_l = (x-k\pi/256)(1+\epsilon_{\mathrm{argred}}) 
  = \hat{y}(1+\epsilon_{\mathrm{argred}})
  \label{eq:sinargrederror1}
\end{equation}
whith, depending on the quadrant, $\sin(\hat{y}) = \pm\sin(x)$ or
$\sin(\hat{y}) = \pm\cos(x)$ and similarly for $\cos(\hat{y})$. This
just means that $\hat{y}$ is the ideal, errorless reduced value.


For simplicity, we define a common upper bound $y_{\max}$ on
$|\hat{y}|$, $|\mathtt{yh}+\mathtt{yl}|$ and $|\mathtt{yh}|$. This
bound takes into account $\epsilon_{argred}$, an $\epsilon_{53}$ just
for the case when it is a bound on $y_h$, and also an error due to the
fact that we used a machine rounding to get $k$, so the reduced value
may exceed $\pi/512$:
\begin{equation}
  y_{\mathrm{maxargred}} = \frac{\pi}{512}(1+\epsilon_{\mathrm{argred2}})
  \label{eq:ymaxsink0}  
\end{equation}
where $\epsilon_{\mathrm{argred2}}$ should be defined precisely (TODO).


\subsection{\texttt{do\_sin\_k\_zero} \label{dosinkzero}}
In the following we will
assume we are in the case $\sin(\hat{y}) = \sin(x)$, (the proof is
identical in the other cases), therefore the relative error that we need
to compute is
\begin{equation}
  \epsilon_{\mathrm{sinkzero}} = \frac{(\mathtt{*psh} + \mathtt{*psl})}{\sin(\mathtt{x})} -1 = \frac{(\mathtt{*psh} + \mathtt{*psl})}{\sin(\hat{y})} -1
\end{equation}


 \begin{lstlisting}[caption={do\_sin\_k\_zero},firstnumber=1]
#define do_sin_k_zero(psh,psl, yh,  yl)            \
do{                                                \
  double yh2 ;	              			   \
  yh2 = yh*yh;					   \
  ts = yh2 * (s3.d + yh2*(s5.d + yh2*s7.d));	   \
  /* (1+ts)*(yh+yl) is an approx to sin(yh+yl) */  \
  /* Now compute (1+ts)*(yh+yl) */                 \
  Add12(*psh,*psl,   yh, yl+ts*yh);	           \
} while(0)						   
\end{lstlisting}

One can see that we almost have the same code as in
\ref{sec:trigo:fastsine}. The difference is that we have as input
a double-double $\mathtt{yh}+\mathtt{yl}$, which is itself an inexact term. 

At Line 4, the error of neglecting $y_l$ and the rounding error in the
multiplication each amount to half an ulp:
  $\mathtt{yh2}=\mathtt{yh}^2(1+\epsilon_{-53})$, 
 with $\mathtt{yh} = (\mathtt{yh}+\mathtt{yl})(1+\epsilon_{-53}) = \hat{y}(1+\epsilon_{\mathrm{argred}})(1+\epsilon_{-53})$

Therefore
\begin{equation}
  \mathtt{yh2}=\hat{y}^2(1+\epsilon_{\mathtt{yh2}})
\end{equation}

with
\begin{equation}
  \maxeps_{\mathtt{yh2}} = (1+\maxeps_{\mathrm{argred}})^2(1+\maxeps_{-53})^3 - 1
\end{equation}

Line 5 is a standard Horner evaluation. Its approximation error is defined by: 
$$
P_{\mathtt{ts}}(\hat{y}) = \frac{\sin(\hat{y})-\hat{y}}{\hat{y}}(1+\epsilon_{\mathrm{approxts}})
$$

This error is computed in Maple as in \ref{sec:trigo:fastsine}, only the interval changes:
$$\maxeps_{\mathrm{approxts}} = \left\Vert \frac{xP_{\mathtt{ts}}(x)}{\sin(x)-x} -1 \right\Vert_{\infty}$$

We also compute $\maxeps_{\mathrm{hornerts}}$, the bound on the relative error due
to rounding in the Horner evaluation thanks to the
\texttt{compute\_horner\_rounding\_error} procedure. This time, this procedure 
takes into account the relative error carried by \texttt{yh2}, which is
$\maxeps_{\mathtt{yh2}}$ computed above.
We thus get the total relative error on \texttt{ts}:

\begin{equation}
  \mathtt{ts} = P_{\mathtt{ts}}(\hat{y})(1+\epsilon_{\mathrm{hornerts}}) = \frac{\sin(\hat{y})-\hat{y}}{\hat{y}}(1+\epsilon_{\mathrm{approxts}})(1+\epsilon_{\mathrm{hornerts}})
  \label{eq:sink0ts}
\end{equation}

The final \texttt{Add12} is exact. Therefore the overall relative error is:

\begin{eqnarray*}
  \epsilon_{\mathrm{sinkzero}} 
  &=& \frac{((\mathtt{yh}\otimes \mathtt{ts}) \oplus \mathtt{yl}) + \mathtt{yh}}{\sin(\hat{y})} -1 \\
  &=& \frac{(\mathtt{yh}\otimes\mathtt{ts} + \mathtt{yl})(1+\epsilon_{-53}) + \mathtt{yh}}{\sin(\hat{y})} -1\\
  &=& \frac{\mathtt{yh}\otimes\mathtt{ts} + \mathtt{yl} + \mathtt{yh}    \ +\  (\mathtt{yh}\otimes\mathtt{ts} + \mathtt{yl}).\epsilon_{-53}}{\sin(\hat{y})} -1\\
\end{eqnarray*}

Define 
\begin{equation}
  \delta_{\mathrm{addsin}} = (\mathtt{yh}\otimes\mathtt{ts} + \mathtt{yl}).\epsilon_{-53}
\label{eq:addsin}
\end{equation}
($\maxdelta_{\mathrm{addsin}}$ may be computed out of the the maximum
values taken by \texttt{ts}, \texttt{yh} and \texttt{yl}). Then we have
\begin{eqnarray*}
  \epsilon_{\mathrm{sinkzero}} 
  &=& \frac{(\mathtt{yh} + \mathtt{yl})\mathtt{ts}(1+\epsilon_{-53})^2 + \mathtt{yl} + \mathtt{yh}    \ +\  \delta_{\mathrm{addsin}} }{\sin(\hat{y})} -1\\
\end{eqnarray*}

Using (\ref{eq:sinargrederror1}) and (\ref{eq:sink0ts}) we get:
\begin{eqnarray*}
  \epsilon_{\mathrm{sinkzero}} 
  &=& \frac{\hat{y}(1+\epsilon_{\mathrm{argred}})\times\frac{\sin(\hat{y})-\hat{y}}{\hat{y}}(1+\epsilon_{\mathrm{approxts}})(1+\epsilon_{\mathrm{hornerts}})(1+\epsilon_{-53})^2 + \mathtt{yl} + \mathtt{yh}    \ +\  \delta_{\mathrm{addsin}} }{\sin(\hat{y})} -1\\
\end{eqnarray*}

To lighten notations, let us define 
\begin{equation}
 \epsilon_{\mathrm{sin1}} = (1+\epsilon_{\mathrm{approxts}})(1+\epsilon_{\mathrm{hornerts}})(1+\epsilon_{-53})^2 \ -\ 1
  \label{eq:epssin1}
\end{equation}

We get
\begin{eqnarray*}
  \epsilon_{\mathrm{sinkzero}} 
  &=& \frac{(\sin(\hat{y})-\hat{y})(1+\epsilon_{\mathrm{sin1}}) + \hat{y}(1+\epsilon_{\mathrm{argred}})    \ +\   \delta_{\mathrm{addsin}} - \sin(\hat{y})}{\sin(\hat{y})}\\
  &=& \frac{(\sin(\hat{y})-\hat{y}).\epsilon_{\mathrm{sin1}} + \hat{y}.\epsilon_{\mathrm{argred}}    \ +\ \delta_{\mathrm{addsin}}}{\sin(\hat{y})}\\
\label{eq:sinkzero}
\end{eqnarray*}

Again we may compute the value of $\maxeps_{\mathrm{sinkzero}}$ as an
infinite norm under Maple. Or, we may compute majorations using
$y_{\mathrm{maxargred}}$.

 



\subsection{\texttt{do\_sin\_k\_notzero}}



\section{Detailed examination of the sine}

The sine begins with casting the high part of the absolute value of
the input number into a 32-bit integer, to enable faster comparisons.
However we have to be aware that we lost the lower part of \texttt{x},
which had a value up to $(2^{31}-1)\ulp(x)$. This is taken care of in
the procedure that converts a Maple high-precision number into an
integer to which \texttt{x} will be compared (procedure
\texttt{outputHighPart} in \texttt{trigo.mpl}).

\begin{lstlisting}[caption={Casting to an int for faster comparisons},firstnumber=1]
  db_number x_split;
  x_split.d=x;
  absxhi = x_split.i[HI_ENDIAN] & 0x7fffffff;
\end{lstlisting}


\subsection{Exceptional cases in RN mode}
\begin{lstlisting}[caption={Exceptional cases for sine RN},firstnumber=1]
  /* SPECIAL CASES: x=(Nan, Inf) sin(x)=Nan */
  if (absxhi>=0x7ff00000) return x-x;    
   
  else if (absxhi < XMAX_SIN_CASE2){
    /* CASE 1 : x small enough sin(x)=x */
    if (absxhi <XMAX_RETURN_X_FOR_SIN)
      return x;
\end{lstlisting}

\subsection{Exceptional cases in RU mode}
\begin{lstlisting}[caption={Exceptional cases for sine RU},firstnumber=1]
  /* SPECIAL CASES: x=(Nan, Inf) sin(x)=Nan */
  if (absxhi>=0x7ff00000) return x-x;    
  
  if (absxhi < XMAX_SIN_CASE2){

    /* CASE 1 : x small enough, return x suitably rounded */
    if (absxhi <XMAX_RETURN_X_FOR_SIN) {
      if(x>=0.)
	return x;
      else {
	x_split.l --;
	return x_split.d;
      }
    }
\end{lstlisting}
\subsection{Exceptional cases in RD mode}
\begin{lstlisting}[caption={Exceptional cases for sine RD},firstnumber=1]
  /* SPECIAL CASES: x=(Nan, Inf) sin(x)=Nan */
  if (absxhi>=0x7ff00000) return x-x;    
  
  if (absxhi < XMAX_SIN_CASE2){

    /* CASE 1 : x small enough, return x suitably rounded */
    if (absxhi <XMAX_RETURN_X_FOR_SIN) {
      if(x<=0.)
	return x;
      else {
	x_split.l --;
	return x_split.d;
      }
    }
\end{lstlisting}
\subsection{Exceptional cases in RZ mode}
\begin{lstlisting}[caption={Exceptional cases for sine RZ},firstnumber=1]
  /* SPECIAL CASES: x=(Nan, Inf) sin(x)=Nan */
  if (absxhi>=0x7ff00000) return x-x;    
  
  if (absxhi < XMAX_SIN_CASE2){

    /* CASE 1 : x small enough, return x suitably rounded */
    if (absxhi <XMAX_RETURN_X_FOR_SIN) {
      x_split.l --;
      return x_split.d;
    }
\end{lstlisting}
\subsection{Fast approximation of sine for small arguments \label{sec:trigo:fastsine}}

\begin{lstlisting}[caption={Sine, case 2},firstnumber=1]
    xx = x*x;
    ts = xx * (s3.d + xx*(s5.d + xx*s7.d ));
    Add12(sh,sl, x, x*ts);
\end{lstlisting}

Here we have had no argument reduction, therefore \texttt{x} is exact.
We need to compute the relative error of $\mathtt{sh}+\mathtt{sl}$
with respect to $\sin(\mathtt{x})$. As $\mathtt{sh}+\mathtt{sl}$ is
the result of an (exact) \texttt{Add12}, the error is:

\begin{equation}
  \epsilon_{\mathrm{sinCase2}} = \frac{\mathtt{x}\otimes \mathtt{ts} + \mathtt{x}}{\sin(\mathtt{x})} -1 = \frac{\mathtt{x}\times\mathtt{ts}(1+\epsilon_{-53}) + \mathtt{x}}{\sin(\mathtt{x})} -1
\label{eq:SinCase2Total}
\end{equation}
The polynomial used to compute \texttt{ts}
approximates $\frac{\sin(x)-x}{x}$: 
$$
P_{\mathtt{ts}}(x) = \mathtt{s3}.x^2 + \mathtt{s5}.x^4 + \mathtt{s7}.x^6
= \frac{\sin(x)-x}{x}(1+\epsilon_{\mathrm{approxts}})
$$

We compute a bound on this error in Maple as 
$$\maxeps_{\mathrm{approxts}} = \left\Vert \frac{xP_{\mathtt{ts}}(x)}{\sin(x)-x} -1 \right\Vert_{\infty}[-x_{\mathrm{max}}...x_{\mathrm{max}}]$$

We also compute $\epsilon_{\mathrm{hornerts}}$, the relative error due
to rounding in the Horner evaluation thanks to the
\texttt{compute\_horner\_rounding\_error} procedure. For this we need
the relative error carried by \texttt{xx}, which is only due to the
rounding error in the multiplication since \texttt{x} is exact:
$$\mathtt{xx}=\mathtt{x}^2(1+\epsilon_{-53})$$

We therefore have:

$$\mathtt{ts} = P_{\mathtt{ts}}(\mathtt{x})(1+\epsilon_{\mathrm{hornerts}}) = \frac{\sin(\mathtt{x})-\mathtt{x}}{\mathtt{x}}(1+\epsilon_{\mathrm{approxts}})(1+\epsilon_{\mathrm{hornerts}})$$

Reporting this in (\ref{eq:SinCase2Total}), we get 
\begin{equation*}
  \epsilon_{\mathrm{sinCase2}} = \frac{(\sin(\mathtt{x})-\mathtt{x})(1+\epsilon_{\mathrm{approxts}})(1+\epsilon_{\mathrm{hornerts}})(1+\epsilon_{-53}) + \mathtt{x}}{\sin(\mathtt{x})} -1
\end{equation*}
or,
\begin{equation*}
  \epsilon_{\mathrm{sinCase2}} =  \frac{\sin(\mathtt{x})-\mathtt{x}}{\sin(\mathtt{x})}\left((1+\epsilon_{\mathrm{approxts}})(1+\epsilon_{\mathrm{hornerts}})(1+\epsilon_{-53})\ -\ 1\right)
\end{equation*}

Finally
\begin{equation}
  \maxeps_{\mathrm{sinCase2}} =  \left\Vert\frac{\sin(\mathtt{x})-\mathtt{x}}{\sin(\mathtt{x})}\right\Vert_{\infty}((1+\maxeps_{\mathrm{approxts}})(1+\maxeps_{\mathrm{hornerts}})(1+\maxeps_{-53})\ -\ 1)
  \label{eq:SinCase2Total2}
\end{equation}
 



\section{Detailed examination of the cosine}
\section{Detailed examination of the tangent}
