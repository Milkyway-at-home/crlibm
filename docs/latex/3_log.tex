
\newcommand{\middlei}{\mathrm{middle}[i]}


\section{Overview}

The worst-case accuracy required to compute a logarithm function
correctly rounded in double precision is $118$ bits according to
Lefèvre and Muller \cite{LefMul2004}.

We therefore proceed in two phases \cite{Ziv91}.  The first, quick
phase (program \texttt{log\_fast.c}) is accurate only to $57-64$ bits,
depending on the execution path as detailed below.  If this is not
enough to decide correct rounding, a second phase, accurate to $120$
bits using the SCS library is launched (program \texttt{log.c}).


\subsubsection*{Definition interval and exceptional cases}

The natural logarithm is defined over positive floating point numbers.  

\begin{itemize}
\item If $x \le 0$ , then $\log(x)$ should return $NaN$
\item If $x = +\infty$ , then $\log(x)$ should return $+\infty$. 
\end{itemize}

This is true in all rounding modes.

Concerning denormals, the smallest exponent of the logarithm of a
double-precision input number is $-53$ (for the input values
$\log(1+2^{-52})$ and $\log(1-2^{-52})$, as $\log(1+\epsilon) \approx
\epsilon$ when $\epsilon\rightarrow 0$). This will make it easy to
ensure that no denormal ever appears in the computation of the
logarithm of a double-precision input number.

\section{Quick phase}

\subsection{Overview of the algorithm}

The algorithm consists of an argument reduction using the well-known
property of the logarithm, and a polynomial evaluation using a degree
12 polynomial.


First we need to handle denormal inputs: If $x < 2^{-1022}$ , ie if x
is a subnormal number, then we use the equation
$$\log(x) = -52 * \log(2) + \log\left(\frac{x}{2^{-52}}\right)$$
where
$\displaystyle \frac{x}{2^{-52}}$ is now a normalized number.


Argument reduction starts with the decomposition of $x$ into its
mantissa $m$ and its exponent $E$. This decomposition can be computed
exactly. The mantissa is in $[1,2[$ and we prefer an interval centered
around $1$, so if needed the mantissa is  divided by 2 (and
the exponent incremented) to get
  
\begin{equation}
x = 2^{E}. y \label{eq:argred}
\end{equation}
where $E$ is an integer, and $y$ satisfies
\begin{equation}
\frac{11}{16}<\frac{\sqrt{2}}{2} < y < \sqrt{2}<\frac{23}{16} \quad.
\end{equation}

The final reconstruction will use the equation
 \begin{equation}
\log(x) = E \times \log(2) + \log(y) \quad.
\end{equation}


The interval $[\frac{11}{16},\frac{23}{16}]$ being too large for a
polynomial approximation of acceptable degree, it is broken down into
8 intervals given in Table~\ref{table:TablePolysLog1}.  Note that the
first four intervals are of size $2^{-4}$, while the last four are of
size $2^{-3}$.  The value of $i$, the index of the interval $X[i]$ to
which $y$ belongs, will be computed out of a few bits of $y$.

Noting $\middlei$ the middle of the $i$-th interval, the final range
reduction consists in computing $z = y - \middlei$. Again this range
reduction can be computed exactly thanks to Sterbenz lemma.  On each
interval, a polynomial $P[i](z)$ approximates $\log(y)$. In the
following $P[i]$ will be noted $P$ when no ambiguity arises.

Each polynomial has coefficients which are exactly representable as
IEEE doubles, with the two first coefficients being exactly
representable as the sum of two doubles: $c_0 = c_0^{hi} + c_0^{lo}$
and $c_1 = c_1^{hi} + c_1^{lo}$. Their relative approximation error is
also given in Table~\ref{table:TablePolysLog1}.


\begin{table}[htdp]\caption{The polynomials and their intervals \label{table:TablePolysLog1}}
\renewcommand{\arraystretch}{1.3}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
polynomial &    interval     &   $\middlei$   & $\max(|z|)$ & $\log_2(\maxrelerr{approx}[i])$\\
\hline  
P[0] & $[\frac{11}{16},\frac{12}{16}]$ &  $\frac{23}{32}$ & $2^{-5}$  &  63.24  \\ 
\hline                                                                                
P[1] & $[\frac{12}{16},\frac{13}{16}]$ &  $\frac{25}{32}$ & $2^{-5}$  &  62.23  \\
\hline                                                                                
P[2] & $[\frac{13}{16},\frac{14}{16}]$ &  $\frac{27}{32}$ & $2^{-5}$  &  61.19  \\ 
\hline                                                                                
P[3] & $[\frac{14}{16},\frac{15}{16}]$ &  $\frac{29}{32}$ & $2^{-5}$  &  60.09   \\ 
\hline                                                                                
P[4] & $[\frac{15}{16},\frac{17}{16}]$ &  $1$             & $2^{-4}$  &  61.72 \\ 
\hline                                                                                
P[5] & $[\frac{17}{16},\frac{19}{16}]$ &  $\frac{18}{16}$ & $2^{-4}$  &  59.33   \\ 
\hline                                                                                
P[6] & $[\frac{19}{16},\frac{21}{16}]$ &  $\frac{20}{16}$ & $2^{-4}$  &  62.34 \\ 
\hline                                                                                
P[7] & $[\frac{21}{16},\frac{23}{16}]$ &  $\frac{22}{16}$ & $2^{-4}$  &  63.03  \\ 
\hline
\end{tabular}\end{center}\end{table}



The polynomials  are evaluated thanks to a Horner scheme:

$$P(z) = c_0^{hi}+c_0^{lo} + z .(c_1^{hi} +c_1^{lo} + z .(c_2 + z
  .(c_3 + ...
 + z . (c_{11} + (c_{12} . z))))))))))))
$$
where the two last iterations may use double-double arithmetic if
required by the overall target accuracy, as detailed below.

Also note that the range of $z$ makes it easy to prove that no
denormal ever appear in the Horner evaluation of a polynomial. This
will be checked by the Maple procedure computing the rounding error of
the Horner scheme.


The reconstruction computes in double-double arithmetic: 
$$\log(x) \approx E\times \log(2) + P(z)$$
where $P(z)$ has been
computed by the previous step as the sum of two double-precision
numbers.  

The computation of $E\times \log(2)$ as a double-double exploits the
fact that $E$ is a small integer: The constant $\log(2)$ is stored as
the sum of two double-precision numbers
$l_h+l_l=\log(2)(1+\epsilon_{-101})$, with $l_h$ having the 11 lower
bits of its mantissa equal to zero, so that the computation of
$E\otimes l_h$ is exact. Now $E\times \log(2)$ may be computed as
Add12$(E\otimes l_h, E\otimes l_l)$.  There the Add12 procedure is
exact as well as the computation of $E\otimes l_h$, so the only errors
are the representation error in $l_l$ and the rounding error in
$E\otimes l_l$, the sum of which amounts to a total relative error
smaller than $\maxrelerr{reconstruction}=2^{-100}$ for the computation of $E\times \log(2)$.


\subsection{Error analysis}

Bounds on the polynomial approximation errors (both relative
$\maxrelerr{approx}[i]$ and absolute $\maxabserr{approx}[i]$) are easily computed under Maple
as infinite norms.

The rounding tests need a bound on the relative error, which can be computed as follows:

\begin{itemize}
\item If $E\ne0$, then the result is computed as $E\times \log(2) +
  P(z)$. We compute a bound on the relative error by dividing the sum
  of all absolute errors in the approximation scheme by the minimum
   value of the result:
  $$\maxrelerr{total} = \frac{\maxabserr{approx} +
    \maxabserr{rounding-poly} +
    \maxabserr{reconstruction}}{\min(|E\log(2)+P(x)|)}$$
  
  Here,
  $\maxabserr{reconstruction}=\max(E\log(2))\times\maxrelerr{reconstruction}$
  as computed previously, and $\maxabserr{approx}$ and
  $\maxabserr{rounding-poly}$ can be computed by generic Maple
  procedures of 
  \ref{section:commonMaple} for several variants of the polynomial evaluation which are
  detailed below.
  
  In this formula one sees several ways to handle the tradeoff between
  accuracy (which translates as percentage of cases where the slow,
  accurate phase will be needed) and performance of the quick phase:
\begin{itemize}
\item The absolute value of $P(z)$ is bounded by $0.37$. A bound on
  the denominator can therefore easily be deduced from the value of
  $|E|$.  In other terms, all things being equal, there will be less
  chances to go to the accurate phase for larger $E$.  It is therefore
  worth adding a test to the code which selects a rounding constant
  according to the value of $|E|$.
  
  The ``normal'' execution path of our algorithm performs the
  Horner polynomial evaluation with the three last operations (two additions
  and one multiplication) performed in double-double arithmetic. This
  leads to two values of $\maxrelerr{total}$: 
  \begin{itemize}
  \item {If $|E| \ge 24$} then we have  $\maxrelerr{total} = 2^{-64.8}$.
  \item {If $0<|E| < 24$} then we have $\maxrelerr{total} = 2^{-60.2}$.
  \end{itemize}
  
\item For even larger values of $|E|$, we can afford to have a large
  $\maxabserr{rounding-poly}$ and still get an acceptable
  $\maxrelerr{total}$. This allows us to evaluate the polynomial
  faster, without using double-double arithmetic. This situation will
  be referred to as the \emph{fast path} in the following. 
  
  Actually, performing only the last Horner addition in double-double
  on the fast path turns out to improve the average performance: It
  slows down the quick phase only a little, and greatly reduces
  $\maxabserr{rounding-poly}$, hence the chance of taking the slow,
  accurate phase. This in turn allows to take the fast path more
  often, \emph{i.e} for more values of $E$.
\end{itemize}

\item If $E=0$ then the reconstruction step is skipped, and the error
  $\maxrelerr{total}$ is given by the Maple procedures of
  \ref{section:commonMaple}. 
  
  It turns out that the polynomial $P[5]$ (the one which is centered
  on $1$ and has therefore a null coefficient of degree 0) leads to a
  relative rounding error which is much worse than that of the other
  polynomials. To still get an acceptable percentage of accurate phase
  in this case, and as the code contains a test if $E=0$ anyway, we
  chose to perform the polynomial evaluation in this case with one
  double-double multiplication more than in the normal case.

\end{itemize}


Actual values for this error analysis are summed up in
Table~\ref{tab:logerror}. In the implementation, we take as rounding
constant the worst case in each column. One remarks that the worst
case for the three first columns is for $P[5]$, which would suggest
yet another improvement: having specific rounding constants for $i=5$.
A closer look  at the average expected improvement shows that the
benefit will be minimal.


\begin{table}[!h]
  \begin{center}
    \begin{tabular}{|c|c||c|c|c|c|}
      \hline
      $P$ & $\infnorm{P}$ &  $E=0$ & $1 \le E < 24$ & $24\le E < 186$ & fast path\\ 
      \hline
P[1] & 0.375 & 61.44 & 60.43& 66.10  63.24\\ 
P[2] & 0.288 & 60.81 & 60.83& 66.16  63.29\\ 
P[3] & 0.208 & 60.02 & 61.11& 66.19  63.14\\ 
P[4] & 0.134 & 58.96 & 61.39& 66.27  63.34\\ 
P[5] & 0.065 & 57.63 & 60.04& 64.76  62.39\\ 
P[6] & 0.172 & 57.94 & 60.15& 65.13  63.01\\ 
P[7] & 0.272 & 59.89 & 60.23& 65.51  63.05\\ 
P[8] & 0.363 & 60.58 & 60.02& 65.65  63.23\\ 
      \hline
      \hline
    \end{tabular}
  \end{center}
  \caption{Error analysis for the various path in the algorithm. The table gives $-\log_2(\maxrelerr{total})$ in the different execution path.}
  \label{tab:logerror}
\end{table}

%%        P[1] & 0.375 & 61.10& 61.26 & 66.93 & 63.30\\ 
%%       \hline                             
%%       P[2] & 0.288 & 60.89& 61.87 & 67.20 & 63.46\\ 
%%       \hline                             
%%       P[3] & 0.208 & 60.47& 62.35 & 67.43 & 63.39\\ 
%%       \hline                             
%%       P[4] & 0.134 & 59.62& 62.77 & 67.65 & 63.73\\ 
%%       \hline                             
%%       P[5] & 0.065 & 57.68& 60.16 & 64.88 & 62.91\\ 
%%       \hline                             
%%       P[6] & 0.172 & 58.12& 61.22 & 66.20 & 62.78\\ 
%%       \hline                             
%%       P[7] & 0.272 & 59.93& 61.22 & 66.50 & 62.94\\ 
%%       \hline                             
%%       P[8] & 0.363 & 60.90& 61.19 & 66.81 & 63.23\\


\newpage
\subsection{Details of computer program}

A procedure \texttt{log\_quick} contains the computation shared by the
three functions \texttt{log\_rn}, \texttt{log\_ru} and
\texttt{log\_rd} (the function \texttt{log\_rz} calls either
\texttt{log\_ru} or \texttt{log\_rd}).  This procedures returns an
approximation to the log as two double-precision numbers, and an index
in an array of constants for testing if correct rounding is possible.
This array contains the relative error for directed rounding modes
(see~\ref{th:roundingDirected} p.~\pageref{th:roundingDirected}) , and
the rounding constant computed as per Theorem~\ref{th:roundingRN1}
p.~\pageref{th:roundingRN1} for round-to nearest.


\newpage
\subsubsection{Exceptional cases and argument reduction}

This part  is shown for \texttt{log\_rn}, but it is identical for the three functions.

\begin{lstlisting}[caption={Exceptional cases},firstnumber=1]
 double log_rn(double x) { 
   db_number y;
   double res_hi,res_lo,roundcst;
   int E,rndcstindex;

   E=0;
   y.d=x;

   /* Filter cases */
   if (y.i[HI_ENDIAN] < 0x00100000){        /* x < 2^(-1022)    */
     if (((y.i[HI_ENDIAN] & 0x7fffffff)|y.i[LO_ENDIAN])==0){
       return -1.0/0.0;     
     }                                     /* log(+/-0) = -Inf */
     if (y.i[HI_ENDIAN] < 0){ 
       return (x-x)/0;                      /* log(-x) = Nan    */
     }
     /* Subnormal number */
     E = -52;           
     y.d *= two52.d;      /* make x a normal number    */ 
   }
    
   if (y.i[HI_ENDIAN] >= 0x7ff00000){
     return  x+x;                                /* Inf or Nan       */
   }
   
   /* reduce to  y.d such that sqrt(2)/2 < y.d < sqrt(2) */
   E += (y.i[HI_ENDIAN]>>20)-1023;                              /* extract the exponent */
   y.i[HI_ENDIAN] =  (y.i[HI_ENDIAN] & 0x000fffff) | 0x3ff00000;        /* do exponent = 0 */
   if (y.d > SQRT_2){
     y.d *= 0.5;
     E++;
   }

   /* Call the actual computation */
   log_quick(&res_hi, &res_lo, &rndcstindex, &y, E);
\end{lstlisting}


\begin{tabular}{ll}
Lines  6,7 &  Initialize E and y\\
Line 10 & Test if x is null, negative or a subnormal number.\\
Line 11,12 & Test if x is $\pm 0$ and return  $-\infty$ \\
Line 14,15 & If x is negative,  return NaN and raise an exception.\\
Line 18,19 & else $x$ is subnormal, then compute:\\
& $log(x) = -52\times log(2) + log(\frac{x}{2^{-52}})$ \\
& (this computation is exact) \\ 
Line 22,23 & If $x$ is $\infty$ or NaN, return $\infty$ or NaN.\\
Line 27 & E contains x 's exponent. \\
Line 28 & y.d is reduced to $\frac{x}{2^E}$. Correct because $y$ was a normal number.\\
Line 29-31 & Now, we have: $ 1 \leq y.d < 2$ and we want $ \frac{1}{\sqrt2} \leq y.d < \sqrt2$.\\
& So, if it's not the case, we update E and y.\\
\end{tabular}


\newpage

\begin{lstlisting}[caption={Procedure \texttt{log\_quick}},firstnumber=1]
static void log_quick(double *pres_hi, double *pres_lo, int* prndcstindex, db_number * py, int E) {
   double ln2_times_E_HI, ln2_times_E_LO, res_hi, res_lo;
   double z, res, P_hi, P_lo;
   int k, i;
   
    /* find the interval including y.d */
    i = ((((*py).i[HI_ENDIAN] & 0x001F0000)>>16)-6) ;
    if (i < 10)
      i = i>>1;
    else
      i = ((i-1)>>1);
    
    z = (*py).d - (middle[i]).d;  /* (exact thanks to Sterbenz Lemma) */
    

    /* Compute ln2_times_E = E*log(2)   in double-double */
    Add12( ln2_times_E_HI, ln2_times_E_LO, ((double)E)*ln2hi.d, ((double)E)*ln2lo.d); 


    /* Now begin the polynomial evaluation of log(1 + z)      */

    res = (Poly_h[i][DEGREE]).d;

    for(k=DEGREE-1; k>1; k--){
      res *= z;
      res += (Poly_h[i][k]).d;
    }

    if((ln2_times_E_HI*ln2_times_E_HI < MIN_FASTPATH*MIN_FASTPATH)) {
      /* Slow path */
      if(E==0) {
        *prndcstindex = 0 ;
        /* In this case we start with a double-double multiplication to get enough relative accuracy */ 
        Mul12(&P_hi, &P_lo, res, z); 
        Add22(&res_hi, &res_lo, (Poly_h[i][1]).d,  (Poly_l[i][1]).d, P_hi, P_lo);
        Mul22(&P_hi, &P_lo, res_hi, res_lo, z, 0.); 
        Add22(pres_hi, pres_lo, (Poly_h[i][0]).d, (Poly_l[i][0]).d, P_hi, P_lo);
      } 
      else
        {
          if((ln2_times_E_HI*ln2_times_E_HI > 16.5*16.5))
            *prndcstindex = 2; 
          else 
            *prndcstindex =1;
          P_hi=res*z;  P_lo=0.; 
          Add22(&res_hi, &res_lo, (Poly_h[i][1]).d,  (Poly_l[i][1]).d, P_hi, P_lo);
          Mul22(&P_hi, &P_lo, res_hi, res_lo, z, 0.); 
          Add22(&res_hi, &res_lo, (Poly_h[i][0]).d, (Poly_l[i][0]).d, P_hi, P_lo);
      
        /* Add E*log(2)  */
          Add22(pres_hi, pres_lo, ln2_times_E_HI, ln2_times_E_LO, res_hi, res_lo);
        }
    }
    else { /* Fast path */
      
      *prndcstindex = 3 ;
      res =   z*((Poly_h[i][1]).d + z*res);
      Add22(&res_hi, &res_lo, (Poly_h[i][0]).d , (Poly_l[i][0]).d, res, 0.);

        /* Add E*log(2)  */
      Add22(pres_hi, pres_lo, ln2_times_E_HI, ln2_times_E_LO, res_hi, res_lo);
    }
}
\end{lstlisting}

\begin{tabular}{ll}
Line 7  & To find the interval $X_i$ containing $y$, we  look upon\\ 
        &   the last bit of the exponent and the 4 first bits of the mantissa.\\
Lines 8-11  & Reduction over $i$ in order to have an index $i$ between 0 and 7\\
        & corresponding to the 8 intervals of Table~\ref{table:TablePolysLog1}.\\
Line 13 & Let us prove that z.d is computed exactly, without rounding error: \\
& \vspace{1ex}To use Sterbenz Lemma, we need to prove that  ${\middlei}/{2} <  y.d < \middlei \times 2$.\\ 
& \vspace{1ex}For every i in $[0;7]$, we have $\middlei-\frac{1}{16} \leq y.d \leq \middlei+\frac{1}{16}$.\\
& \vspace{1ex}Table~\ref{table:TablePolysLog1} gives: $\frac{25}{32} \leq {\middlei} \leq \frac{22}{16}$\\ 
& \vspace{1ex}Therefore ${\middlei}/{2} \leq \frac{22}{32} < \frac{23}{32} \leq \middlei - \frac{1}{16} \leq y.d$\\
& and $y.d \leq \middlei + \frac{1}{16} \leq \frac{23}{16} < \frac{25}{16} \leq \middlei\times 2$\\
Line 17 & The computation of $E\log(2)$ is done as early as possible. \\
        & As stated previously, the relative error here is smaller than $2^{-100}$\\
Lines 22-27 & Begin of Horner evaluation in double arithmetic\\
Line 29 & This test is equivalent to $|ln2\_times\_E_HI| < \mathtt{MIN\_FASTPATH}$.\\
        & As $|E|$ is an integer smaller than $2048$, neither overflow nor denormal can occur.\\
        & Currently $\mathtt{MIN\_FASTPATH}=128.5$ but the Maple programs in \texttt{coef\_log.mw}\\
        & allow to change this value.\\
Lines 31-53 & Slow path, divided two cases:\\
Lines 31-38 & ~Case $E=0$: in this case, there is no addition of $E\log(2)$ in the end, \\
            & ~~~to reach the required relative accuracy we do two $\times$ and two $+$ in double double\\
Lines 40-52 & ~Case $E\ne 0$ Thanks to the final addition to $E\log(2)$,\\
            & ~~we have to bound only the absolute accuracy of the polynomial evaluation\\
            & ~~which allows to save the first Mult22.\\
Lines 54-62 & Fast path : the polynomial evaluation is entirely in double-precision, and the \\
            & ~ final addition with a large  $E\log(2)$ scales the resulting bad absolute accuracy\\
            & ~ to acceptable relative accuracy\\ 
Lines 34,36,47 & None of the intermediate values reaches  $2^{970}$ so we may use the unconditional Mul22.\\
Lines 35,37,46,48 & The Maple procedure checks that the first argument is always greater \\
                  & than the second, so we may use the unconditional Add22.\\
Lines 51,61 & $E\log(2)$ is greater than  $\log(2)\approx 0.69$, and $\infnorm{P_i(x)}<0.4$\\
                  & so we may use the unconditional Add22.\\

\end{tabular}



\subsection{Rounding}

\subsubsection{Rounding to the nearest}

The code for rounding is strictly identical to that of
Theorem~\ref{th:roundingRN1}.  The condition to this theorem that
$\mathtt{res\_hi}\ge 2^{-1022+53}$ is ensured by the image domain of
the $\log$ over the floating-point numbers: as already mentionned, the
smallest possible value of the logarithm of a floating-point number is
larger than $2^{-53}$. As $\mathtt{res\_hi}$ approximates this value
with a relative accuracy much smaller than $1$, the condition is
ensured.


\subsection{Directed rounding}

Here again, the code is strictly identical to that of
Theorem~\ref{th:roundingDirected}, and the conditions to this theorem
are ensured by the image domain of the log.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Accurate phase}


There is only one accurate function, called is \texttt{scs\_log}. It
returns an SCS number, which is rounded in the required mode by one of
\texttt{scs\_get\_d}, \texttt{scs\_get\_d\_pinf} and
\texttt{scs\_get\_d\_minf} in the corresponding functions in
\texttt{log\_fast.c}.


\subsection{Argument reduction}

The first argument reduction is the same as in the quick phase.
Therefore the function  \texttt{scs\_log}  take as arguments $y$ and $E$, computed
in the quick phase (similarly, exceptional cases are not considered
again).

As in the quick phase, we will compute the log using $$\log(x) = E \times log(2) + log(1+f)\quad .$$

Now we define $w_i = 1 + i\times2^{-4}$, for $i = -6 ... 6$, and we select the $w_i$ closest to $1+f$, in order to have: 

$$log(1+f) = log(w_i) + log(1+\frac{1+f-w_i}{w_i})$$

where $r=\frac{1+f-w_i}{w_i} \leq 2^{-5}$.

The values of $log(w_i)$, $1/w_i$ and $log(2)$ are tabulated. We
therefore compute the reduced argument $R = (1+f-w_i)/w_i$ by a
subtraction in double precision (exact thanks to Sterbenz Lemma)
followed by a conversion to SCS (also exact) and an SCS multiplication
(relative error smaller than $2^{-207}$ including the error in tabulating $1/w_i$).

\begin{lstlisting}[caption={Argument reduction},firstnumber=1]
void scs_log(scs_ptr res, db_number y, int E){ 
  scs_t R, sc_ln2_times_E, res1, addi;
  scs_ptr ti, inv_wi;
  db_number z, wi;
  int i;

 /* to normalize y.d and round to nearest      */
  /* + (1-trunc(sqrt(2.)/2 * 2^(4))*2^(-4) )+2.^(-(4+1))*/ 
  z.d = y.d + norm_number.d; 
  i = (z.i[HI_ENDIAN] & 0x000fffff);
  i = i >> 16; /* 0<= i <=11 */
  

  wi.d = ((double)(11+i))*0.0625;

  /* (1+f-w_i) */
  y.d -= wi.d; 
  
  /* Table reduction */
  ti     = table_ti_ptr[i]; 
  inv_wi = table_inv_wi_ptr[i];
   
  /* R = (1+f-w_i)/w_i */
  scs_set_d(R, y.d);
  scs_mul(R, R, inv_wi);

\end{lstlisting}




\subsection{Polynomial approximation}

$log(1+\frac{1+f-w_i}{w_i})$ is approximated by a polynomial
$Q(\frac{1+f-w_i}{w_i})$ with an overall relative error less than $2^{-130}$.

The polynomials are available in \texttt{log.h}.


\begin{lstlisting}[caption={Polynomial approximation},firstnumber=1]
  scs_mul(res1, constant_poly_ptr[0], R);
  for(i=1; i<20; i++){
    scs_add(addi, constant_poly_ptr[i], res1);
    scs_mul(res1, addi, R);
  }
\end{lstlisting}

TODO:
Add a Maple script that computes the relative error of a polynomial given an error on the input. It is required here.




\subsection{Reconstruction}
At the end, we compute:

\begin{equation}result = E\times log(2) + log(w_i) + Q(\frac{1+f-w_i}{w_i})\end{equation}

\begin{lstlisting}[caption={Reconstruction},firstnumber=1]
  if(E==0){
    scs_add(res, res1, ti);
  }else{
    /* sc_ln2_times_E = E*log(2)  */
    scs_set(sc_ln2_times_E, sc_ln2_ptr);

    if (E >= 0){
      scs_mul_ui(sc_ln2_times_E, (unsigned int) E);
    }else{
      scs_mul_ui(sc_ln2_times_E, (unsigned int) -E);
      sc_ln2_times_E->sign = -1;
    }
    scs_add(addi, res1, ti);
    scs_add(res, addi, sc_ln2_times_E); 
  }
}
\end{lstlisting}



\subsection{Rounding}

As already stated, the procedure \texttt{scs\_log} returns an SCS number which is rounded
in the required mode by the caller function. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analysis of the logarithm performance}
\label{section:log_results}
The input numbers for the performance tests given here are random
positive double-precision numbers with a normal distribution on the
exponents. More precisely, we take random 63-bit integers and cast
them into double-precision numbers.  


In average, the second step is taken in 0.23\% of the calls, which
seems a rather good balance considering the respective costs of the
first and second steps (seen in the table as the min and max times,
respectively).


\subsection{Speed}
Table \ref{tbl:log_abstime} (produced by the \texttt{crlibm\_testperf}
executable) gives absolute timings for a variety of processors and
operating systems.  

\begin{table}[!htb]
\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|l|r|r|r|}
\hline
 \multicolumn{4}{|c|}{Pentium 4 Xeon / Linux Debian sarge / gcc 3.3}   \\ 
 \hline
                         & min time      & max time      & avg time \\ 
 \hline
 \texttt{libm}           & 724          & 9808          &        733 \\ 
 \hline
  \texttt{mpfr}          & 496          & 288676        &      84955 \\ 
 \hline
  \texttt{libultim}      & 784          & 524756        &       1049 \\ 
 \hline
 \texttt{crlibm}         & 896          & 60832         &       1080 \\ 
 \hline
 \hline
 \multicolumn{4}{|c|}{Pentium III / Linux Debian woody / gcc 3.2}   \\
 \hline
                         & min time      & max time      & avg time \\
 \hline
 \texttt{libm}           & 387          & 1138          &        390 \\
 \hline
  \texttt{mpfr}          & 431          & 315982        &      92250 \\
 \hline
  \texttt{libultim}      & 376          & 217005        &        534 \\
 \hline
 \texttt{crlibm}         & 383          & 22861         &        634 \\
 \hline
 \hline
  \multicolumn{4}{|c|}{PowerPC G4 / MacOS X / gcc2.95}   \\
 \hline
                         & min time      & max time      & avg time \\
 \hline
 \texttt{libm}           & 14           & 16            &         15 \\
 \hline
  \texttt{mpfr}          & 4610         & 8620          &       4895 \\
 \hline
  \texttt{libultim}      & 20           & 19890         &         22 \\
 \hline
 \texttt{crlibm} (without FMA)      & 5            & 1241          &         32 \\
\hline
 \texttt{crlibm} (using FMA)        & 5           & 1144          &         24 \\

\hline
\end{tabular}
\end{center}
\caption{Absolute timings for the logarithm (arbitrary units)
  \label{tbl:log_abstime}}
\end{table}

Contributions to this table for new processors/OS/compiler combinations are welcome.



\subsection{Memory requirements}

Table size is
\begin{itemize}
\item for the \quick\ phase,
  $8\times 15\times8=960$ bytes for the eight polynomials, plus
  another $64$ bytes for the rounding constants, or a total of exactly
  1kB.
\item for the \accurate\ phase, $1+13+13+20$ SCS constants:  $\log(2)$,
  the 13 $log(w_i)$ and the 13 $\frac{1}{w_i}$,  the polynomial of
  degree 20 with a null first coefficient. This amounts to a little more than 2kB.
\end{itemize}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and perspectives}


In the log we have a fairly good balance between both evaluation
phases, which was obtained thanks to automated testing of various
sceniarii. The main lesson learned here was that designing a rigorous
proof along with the code helped tuning performance, by helping
managing the tradeoff between speed and accuracy of the first path.

Our log is a few percent slower in average than Ziv/IBM's, but if we
inline the code of \texttt{log\_quick} then it becomes faster than
Ziv's: Inlining saves a function call, but also allows other minor
improvements, like removing the need for the constant index, and
loading the constants for the rounding test in advance to hide their loading
time. However, as it increases the size of the code and degrades its
maintainability, we believe the current approach makes more sense in
\crlibm.

It might be interesting for some applications to add a fast execution
paths for values around $1$, as Ziv does: This is the situation
where our quick phase is slowest and our second phase most often
taken. However it will not show any improvement in our test protocol.
Besides, it is probably more important to implement the standard $\log(1+x)$
function, which is designed specifically for this case.

Finally, we did not try to implement the argument reduction used in
the accurate phase for the quick phase, mostly because it is not
exact (contrary to the one we used), which complicates error analysis. This is, however, another
obvious path to explore.

