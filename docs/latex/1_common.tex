% Common notations, theorems and procedures

\section{Notations\label{section:notations}}

Throughout the paper, we will note, and  .





The following notations will be used throughout this document:
\begin{itemize}

\item  $+$, $-$ and  $\times$ denote the usual
mathematical operations.

\item $\oplus$, $\ominus$ and $\otimes$ denote the
corresponding floating-point operations in IEEE-754 double precision,
in the IEEE-754 \emph{round to nearest} mode.

\item $\epsilon$ (usually with some index) denotes an error and
  $\delta$ (with the same index) denotes a bound on this error.

\item $\epsilon_{-k}$ -- with a negative index -- denotes an error bounded by $2^{-k}$.
  
\item For a floating-point number $x$, the value of the least
  significant bit of its mantissa is classically denoted $\ulp(x)$.

\end{itemize}




\section{Common C procedures \label{section:commonC}}

\subsection{Sterbenz Lemma}

\begin{theorem}[Sterbenz Lemma~\cite{Ste74,Gol91}]
\label{sterbenz}
If $x$ and $y$ are floating-point numbers, and if ${y}/{2} \leq x \leq 2y$ then $x\ominus y$ is computed exactly, without any rounding error.
\end{theorem}


\subsection{Conversions}

\subsubsection{Double-precision numbers in memory\label{section:memory}}

A double precision floating-point number uses $64$ bits that is two
times the size of an integer usually represented with $32$ bits. The
order in which are stored in memory the two $32$ bits words depends on
the architecture. An architecture is said \emph{Little Endian} if the
lower part of the number is stored in memory at the smallest address;
x86 processor use this representation. Conversely, an architecture
with the high part of the number stored in memory at the smallest
address is said \emph{Big Endian}; PowerPC processor use this
representation.


The following code extract from a double precision number $x$ its
upper, and lower part.

\begin{lstlisting}[label={chap0:lst:endian},
  caption={Extract upper and lower part of a double precision number $x$},firstnumber=1]
/* LITTLE_ENDIAN/BIG_ENDIAN are defined by the user or  */
/* automatically by tools such as autoconf/automake.   */

#ifdef LITTLE_ENDIAN
#define HI(x) *(1+(int*)&x)
#define LO(x) *(int*)&x
#elif BIG_ENDIAN
#define HI(x) *(int*)&x
#define LO(x) *(1+(int*)&x)
#endif
\end{lstlisting}



\subsubsection{Conversion from floating-point to integer}

\begin{theorem}[Conversion floating-point to integer \cite{AMD01}]
  The following algorithm convert a floating-point number $d$ into an
  integer $i$ with rounding to nearest mode.

\begin{lstlisting}[label={chap0:lst:conversion2},caption={Solution 2},firstnumber=1]
#define DOUBLE2INT(i, d) \
  {double t=(d+6755399441055744.0); i=LO(t);}
\end{lstlisting}
\end{theorem}
This algorithm adds the constant $2^{52}+2^{51}$ to the floating-point
number to put the integer part of $x$, in the lower part of the
floating-point number.  We use $2^{52}+2^{51}$ and not $2^{52}$,
because the value $2^{51}$ is used to contain possible carry
propagations with negative numbers.



\subsection{Algorithms for double-double arithmetic}

In this section, we give the basic algorithms for computing with
numbers represented the sum of two floating-point numbers (or
\emph{double-double} numbers). The algorithms are given as plain C
functions, but it may be preferable, for performance issue, to
implement them as macros or inlined functions. This implementation
decision depends on the ability of the compiler.


\subsubsection{Exact sum algorithm {Add12}}

This algorithm is also known as the Fast2Sum algorithm in the litterature.
\begin{theorem}[Exact sum~\cite{Knu73, Boldo2001}]
  Let $a$ and $b$ be floating-point numbers, then the following method
  computes two floating-point numbers $s$ and $r$, such that $s+r =
  a+b$ exactly, and $s$ is the floating-point number which is closest
  to $a+b$.

\begin{lstlisting}[label={lst:Add12Cond},caption={Add12Cond},firstnumber=1]
void Add12Cond(double *s, double *r, a, b) 
{
  double z;
  s = a + b;            
  if ((HI(a)&0x7FF00000)> (HI(b)&0x7FF00000)){  
    z = s - a;           
    r = b - z;           
  }else {                 
    z = s - b;           
    r = a - z;           
  } 
}                         
\end{lstlisting}
This algorithm requires $3$ floating-point additions, $2$ masks and $1$ test over integer.
\end{theorem}


If we are able to prove that  the exponent of $a$ is always greater than that
of $b$, then the previous algorithm to perform an exact addition of 2
floating-point numbers becomes :
\begin{lstlisting}[label={lst:Add12},caption={Add12},firstnumber=1]
void Add12Cond(double *s, double *r, a, b) 
{
  double z;
  s = a + b;            
  z = s - a;  
  r = b - z; 
}            
\end{lstlisting}
The cost of this algorithm is $3$ floating-point additions.






\subsubsection{Exact product algorithm {Mul12}}

This algorithm is also known as the Dekker algorithm.

\begin{theorem}[Exact product \cite{Dek71,Knu73}]
  Let $a$ and $b$ be two floating-point numbers, with $p \geq 2$ the
  size of their mantissa. Let $c=2^{\frac{\lceil p \rceil}{2}}+1$. The
  following method computes the two floating-point numbers $r1$ and
  $r2$ such that $r1 + r2 = a + b$ with $r1 = a \otimes b$ in the case
  $p=53$ (double precision):


\begin{lstlisting}[label={lst:Mul12Cond},caption={Mul12Cond},firstnumber=1]
void Mul12Cond(double *r1, double *r2, double a, double b){
  double two_m53 = 1.1102230246251565404e-16;  /* 0x3CA00000, 0x00000000 */
  double two_53  = 9007199254740992.;          /* 0x43400000, 0x00000000 */
  double c        = 134217729.;                /* 0x41A00000, 0x02000000 */ 
  double u, up, u1, u2, v, vp, v1, v2, r1, r2;

  if (HI(a)>0x7C900000) u = a*two_m53; 
  else            u = a;
  if (HI(b)>0x7C900000) v = b*two_m53; 
  else            v = b;

  up = u*c;        vp = v*c;
  u1 = (u-up)+up;  v1 = (v-vp)+vp;
  u2 = u-u1;       v2 = v-v1;
  
  *r1 = u*v;
  *r2 = (((u1*v1-*r1)+(u1*v2))+(u2*v1))+(u2*v2)

  if (HI(a)>0x7C900000) {*r1 *= two_e53; *r2 *= two_53;} 
  if (HI(b)>0x7C900000) {*r1 *= two_e53; *r2 *= two_53;} 
} 
\end{lstlisting}

We have to test $a$ and $b$ before and after the core of the
algorithms in order to avoid overflow by multiplying by $c$. The
global cost in the worst case is $4$ tests over integers, $10$
floating-point additions and $13$ floating-point multiplications.
\end{theorem}

If we are able to prove that $a$ and $b$ are less then $2^{970}$, we can skip
this test, and get the following algorithm:
\begin{lstlisting}[label={lst:Mul12},caption={Mul12},firstnumber=1]
void inline Dekker(double *r1, double *r2, double u, double v){
  double c        = 134217729.;                /* 0x41A00000, 0x02000000 */ 
  double up, u1, u2, vp, v1, v2;

  up = u*c;        vp = v*c;
  u1 = (u-up)+up;  v1 = (v-vp)+vp;
  u2 = u-u1;       v2 = v-v1;
  
  *r1 = u*v;
  *r2 = (((u1*v1-*r1)+(u1*v2))+(u2*v1))+(u2*v2)
}
\end{lstlisting}
which reduces the cost of this algorithm to $10$ floating-point
additions and $7$ floating-point multiplications.







\subsubsection{Double-double addition {Add22}}
  
This algorithm computes the sum of two double-double numbers as a double-double.

\begin{lstlisting}[label={Add22},caption={Add22},firstnumber=1]
void Add22( double * z, double * zz, double x, double xx, double y, double yy){
double r,s;

r = x+y;
s = (ABS(x) > ABS(y))? (x-r+y+yy+xx) : (y-r+x+xx+yy);
*z = r+s;
*zz = r - (*z) + s;
}
\end{lstlisting}

Here ABS is a macro that returns the absolute value of a floating-point number.


\subsubsection{Double-double multiplication {Mul22}}

\begin{lstlisting}[label={Mul22},caption={Mul22},firstnumber=1]
void Mul22(double x, double xx, double y, double yy, double * z, double * zz){
  double c, cc;
  
  Mul12(&c, &cc, x, y);
  cc = x*yy + xx*y + cc;
  *z = c+cc;
  *zz = c - (*z) + cc;
}  
\end{lstlisting}





\subsection{Test if rounding is possible\label{section:testrounding}}

We assume here that an evaluation of $y=f(x)$ has been computed with a
total relative error smaller than $\delta$, and that the result is
available as the sum of two non-overlapping floating-point numbers
$y_h$ and $y_l$ (as is the case if computed by the previous
algorithms). This section gives and proves algorithms for testing if
$y_h$ is the correctly rounded value of $y$.


%% \subsubsection{Rounding to the nearest}
%% % All this commented out, because of the  improved test below
%% \begin{theorem}[Correct rounding of a double-double to a double]
%%   Let $y$ and $\delta$ be real numbers, and $e$, $y_h$ and $y_l$ be
%%   floating-point numbers such that 
%%   \begin{itemize}
%%   \item $y_h=y_h\oplus y_l$,
%%   \item none of $y_h$ and $y_l$ is a  NaN.
%%   \item $|y_h+y_l - y| < \delta.|y_h|$
%%   \item $0< \delta \le 2^{-54}$
%%   \item $e\ge 1+ 2^{55}\delta$ 
%% \end{itemize}

%% The following test determines whether $y_h$ is the
%%   correctly rounded value of $y$ in  round to nearest mode.

%% \begin{lstlisting}[label={roundingtonearest},
%%   caption={Test for correct rounding to nearest},
%%   firstnumber=1]
%% if( (*@$y_h$@*) == ((*@$y_h$@*) + ((*@$y_l$@*)*e)) )
%%   return (*@$y_h$@*);
%% else /* more accuracy is needed, lauch accurate phase */
%% \end{lstlisting}
%% \end{theorem}

%% \begin{proof}
%% %%   The intuition here is the following: the exact value $y=f(x)$ is in
%% %%   an interval $](y_h+y_l)(1-\delta),(y_h+y_l)(1+\delta)[$.  The
%% %%   floating-point number $e$ is precomputed such that we have a
%% %%   guarantee that $y$ is in the interval $[y_h+y_l, y_h+y_l\otimes e]$
%% %%   (or $[y_h+y_l\otimes e, y_h+y_l]$ if $y_l$ is negative).  By
%% %%   hypothesis we know that $\round(y_h + y_l) = y_h$.  The test tries
%% %%   to round the other bound of the interval.  If the rounding of this
%% %%   bound is also $y_h$, then thanks to the monotonicity of the rounding
%% %%   function, it is also the rounding of all the points in the interval,
%% %%   including $y$.
  
%% %%   We now give a more formal proof.
  
%%   The property holds if $y_h$ is $\pm \infty$.
  
%%   In the following we assume that $y_h\ge0$, as the other
%%   case is symmetric.
  
%%   Let us note  $u=\ulp(y_h)$. By definition of the \ulp,
%%   we have $y_h \in [2^{52}u, (2^{53}-1)u]$, which implies $y_h < 2^{53}u$. 


%% %% The error bound
%% %%   $|y_h+y_l - y| < \delta |y_h|$ therefore implies $|y_h+y_l - y| <
%% %%   \delta |y_h|$.

  
%%   What we want to prove is that if the test is true, then $y_h =
%%   \round(y)$. We will prove that $|y_l-\frac{1}{2}u| \ge 2^{53}\delta
%%   u$, which implies $|y_l-\frac{1}{2}u| \ge \delta y_h$, which implies
%%   that we are not in a difficult case for correct rounding to the
%%   nearest.

%%   Suppose that the test is true ($y_h \oplus y_l \otimes e = y_h$). 
%%   With IEEE-54 compliant rounding to
%%   nearest, this implies $|y_l \otimes e|
%%   \le \frac{1}{2}u$, which in turn implies $|y_l \times e|
%%   (1-2^{-53}) \le \frac{1}{2}u$. 
  
%%   Consider the case when $y_l$ is positive:  $0\le y_l \le \frac{1}{2}u$.
%%   We get $y_l \times (1+e-1)(1-2^{-53}) \le \frac{1}{2}u$, or
%%   \begin{equation}
%%   \frac{1}{2}u - y_l \ge y_l (e-1)(1-2^{-53})\label{eq:prooftestRN}
%%   \end{equation}  
%%   Now if $0 \le y_l < \frac{1}{4}u$, then since $|\delta|<2^{-54}$ we
%%   are in an easy case for rounding to the nearest, and $y_h =
%%   \round(y)$. If $y_l \ge \frac{1}{4}u$, then (\ref{eq:prooftestRN})
%%   implies $\frac{1}{2}u - y_l \ge \frac{1}{4}u (e-1)(1-2^{-53}) >
%%   2^{53}\delta u$ (since  $e-1 \ge 2^{55}\delta$).
  
%%   The case when $y_l$ is negative is similar.
%% \end{proof}

%% Note that a fused multiply-and-add may be used safely for the
%% computation of $y_h+y_l\times e$.


\subsubsection{Rounding to the nearest}

\begin{theorem}[Correct rounding of a double-double to a double]
  Let $y$ and $\delta$ be real numbers, and $e$, $y_h$ and $y_l$ be
  floating-point numbers such that 
  \begin{itemize}
  \item $y_h=y_h\oplus y_l$,
  \item none of $y_h$ and $y_l$ is a  NaN.
  \item $|y_h+y_l - y| < \delta.|y|$
  \item $0< \delta \le 2^{-53-k}$ with $k\ge 2$ integer
  \item $e\ge 1+  \dfrac{2^{53+k+1}\delta}{(2^{k}-1)(1-2^{-53})}$
\end{itemize}

The following test determines whether $y_h$ is the
  correctly rounded value of $y$ in  round to nearest mode.

\begin{lstlisting}[label={roundingtonearest},
  caption={Test for correct rounding to nearest},
  firstnumber=1]
if( (*@$y_h$@*) == ((*@$y_h$@*) + ((*@$y_l$@*)*e)) )
  return (*@$y_h$@*);
else /* more accuracy is needed, lauch accurate phase */
\end{lstlisting}
\end{theorem}

\begin{proof}
%%   The intuition here is the following: the exact value $y=f(x)$ is in
%%   an interval $](y_h+y_l)(1-\delta),(y_h+y_l)(1+\delta)[$.  The
%%   floating-point number $e$ is precomputed such that we have a
%%   guarantee that $y$ is in the interval $[y_h+y_l, y_h+y_l\otimes e]$
%%   (or $[y_h+y_l\otimes e, y_h+y_l]$ if $y_l$ is negative).  By
%%   hypothesis we know that $\round(y_h + y_l) = y_h$.  The test tries
%%   to round the other bound of the interval.  If the rounding of this
%%   bound is also $y_h$, then thanks to the monotonicity of the rounding
%%   function, it is also the rounding of all the points in the interval,
%%   including $y$.
  
%%   We now give a more formal proof.
  
  The property holds if $y_h$ is $\pm \infty$.
  
  In the following we assume that $y\ge0$ (so $y_h\ge0$), as the other
  case is symmetric.
  
  Let us note $u=\ulp(y_h)$. By definition of the \ulp, we have $y_h
  \in [2^{52}u, (2^{53}-1)u]$, which implies $y<2^{53}u$ as $y < y_h + y_l + \delta y
  < (2^{53}-1)u +\frac{1}{2}u +\frac{1}{2}u$.


%% The error bound
%%   $|y_h+y_l - y| < \delta |y_h|$ therefore implies $|y_h+y_l - y| <
%%   \delta |y_h|$.

  
  What we want to prove is that if the test is true, then $y_h =
  \round(y)$. We will prove that  if the test is true, then $|y_l-\frac{1}{2}u| > 2^{53}\delta
  u$, which implies $|y_l-\frac{1}{2}u| > \delta y$, which implies
  that we are not in a difficult case for correct rounding to the
  nearest.

  Suppose that the test is true ($y_h \oplus y_l \otimes e = y_h$). 
  With IEEE-54 compliant rounding to
  nearest, this implies $|y_l \otimes e|
  \le \frac{1}{2}u$, which in turn implies $|y_l \times e|
  (1-2^{-53}) \le \frac{1}{2}u$. 
  
  Consider the case when $y_l$ is positive:  $0\le y_l \le \frac{1}{2}u$.
  We get $y_l \times (1+e-1)(1-2^{-53}) \le \frac{1}{2}u$, or
  \begin{equation}
  \frac{1}{2}u - y_l \ge y_l (e-1)(1-2^{-53})\label{eq:prooftestRN2}
  \end{equation}  
  Now
  \begin{itemize}
  \item if $0 \le y_l < (\frac{1}{2} - \frac{1}{2^{k+1}})u$, then since
    $|\delta|<2^{-53+k}$ we are in an easy case for rounding to the
    nearest, and $y_h = \round(y)$.
  \item If $y_l \ge (\frac{1}{2} - \frac{1}{2^{k+1}})u =
    \frac{2^{k}-1}{2^{k+1}}u$, then (\ref{eq:prooftestRN2}) implies
    $\frac{1}{2}u - y_l \ge \frac{2^{k}-1}{2^{k+1}} (e-1)(1-2^{-53}) >
    2^{53}\delta u$, from $e\ge 1+
    \frac{2^{53+k+1}\delta}{(2^{k}-1)(1-2^{-53})}$.
\end{itemize}  
  The case when $y_l$ is negative is similar.
\end{proof}



\subsubsection{Rounding up}

TODO

\subsubsection{Rounding down}

TODO







\section{The Software Carry Save library}







\section{Common Maple procedures \label{section:commonMaple}}


\subsection{Conversions}

Procedure \texttt{nearest} provides the closest IEEE-double number
from input value \texttt{u}.

\begin{lstlisting}[caption={nearest},firstnumber=1]

nearest := proc(u)
local arrondi, signe, x, exposant, mantisseinfinie, mantisse;
Digits:=200:
if u = 0 then arrondi := 0
else 
  if (u < 0) then signe := -1; x := -u else signe := 1; x := u fi;
  exposant := floor(evalf(log(x)/log(2.0)));
  mantisseinfinie := x*2^(52-exposant);
  if frac(mantisseinfinie) < 0.5 then mantisse := round(mantisseinfinie)
   else
     mantisse := floor(mantisseinfinie);
     if type(mantisse,odd) then mantisse := mantisse+1 fi;
   fi;
   arrondi := signe*mantisse*2^(exposant-52)
fi;
arrondi;
end:
\end{lstlisting}



Procedure \texttt{IEEEdouble} returns the exponent, the mantissa and the
binary conversion of the closest IEEE-double number of input value \texttt{x}.

\begin{lstlisting}[caption={IEEEdouble},firstnumber=1]
IEEEdouble:=proc(x) local signe, logabsx, exposant, mantisse, mantisseinfinie, resultat; 
 Digits:=200;
if (x=0) then [0,0,0]; 
else 
 if (x<0) then signe:=-1:
 else signe:=1:
 fi:
 exposant := floor(evalf(log(signe*x)/log(2.0)));
 if (exposant>1023) then mantisse:=infinity:
 elif (exposant<-1022) then mantisse:=0:
 else 
  mantisseinfinie := signe*x*2^(52-exposant);
  if frac(mantisseinfinie) <
0.5 then mantisse := round(mantisseinfinie)
   else
     mantisse := floor(mantisseinfinie);
     if type(mantisse,odd) then mantisse := mantisse+1 fi;
   fi;
 mantisse:= signe*mantisse*2**(-52);
 Digits := 53;
 resultat:=convert(mantisse*2.0^(exposant),binary);
 fi;
[exposant, mantisse, resultat];
fi;
end:
\end{lstlisting}

Procedure \texttt{hi\_lo} returns two IEEE-double numbers $x\_hi$ and $x\_lo$ so that $x = x\_hi + x\_lo + \epsilon_{-103}$.

\begin{lstlisting}[caption={hi\_lo},firstnumber=1]

hi_lo:= proc(x)
global x_hi, x_lo, res:
local tamp:
x_hi:= nearest(evalf(x)):
res:=x-x_hi:
if (res = 0) then
  x_lo:=0:
else
  x_lo:=nearest(evalf(res)):
end if;
(x_hi,x_lo);
end:
\end{lstlisting}
\vspace{0.5cm}



Procedure "ieee2Hexa" returns the closest IEEE-double number from x, in hexadecimal.

\begin{lstlisting}[caption={ieee2Hexa},firstnumber=1]

ieee2Hexa:= proc(x)
  local signe, hex1, hex2, ma, manti, expo, expos, bina, bin1, bin2, dec1, dec2;
  if(x=0) then resultat:=["00000000","00000000"];
  elif(x=-0) then resultat:=["80000000","00000000"];
  elif(x=2) then resultat:=["40000000","00000000"];
  elif(x=-2) then resultat:=["C0000000","00000000"];
  else
   ma:=IEEEdouble(x);
   expo:=ma[1]:
   if (ma[2]<0) then 
    manti:=2**64 + 2**63+(-ma[2]-1)*2**52+(expo+1023)*2**52;
   else 
     manti:=2**64 + (ma[2]-1)*2**52+(expo+1023)*2**52;
   fi:
   hex2:=convert(manti, hex); 
   hex2:=convert(hex2, string):  
  
   resultat:=[substring(hex2,2..9), substring(hex2,10..18)];
  end if;
  resultat;
end proc:
\end{lstlisting}
\vspace{0.5cm}

Procedure "Hexa2ieee" returns the decimal IEEE-double number associated with the hexadecimal enter value "hexa".

\begin{lstlisting}[caption={Hexa2ieee},firstnumber=1]

Hexa2ieee:= proc(hexa)
local dec, bin, expobin, expo, mantis, sign, hex1, hex2, hexcat;
global res;

hex1:= op(1, hexa):
hex2:= op(2, hexa):
hexcat:= cat(hex1, hex2);
dec:= convert(hexcat, decimal, hex):

if(dec >= 2**63) then
  dec = dec - 2**63:
  sign:= -1:
else
  sign:= 1:
fi;  
expo:= trunc(dec/(2**52))-1023:
mantis:= 1+frac(dec/(2**52));
res:= evalf(sign*2**(expo)*mantis);
end proc:
\end{lstlisting}


\subsection{Procedures for polynomial approximation}


Procedure \texttt{Poly\_exact2} takes in arguments a polynomial \texttt{P} and a
integer \texttt{n}. It returns a truncated polynomial, of wich coefficients
are exactly IEEE-double numbers. The \texttt{n} first coefficients are written
over 2 IEEE-double numbers.
 

\begin{lstlisting}[caption={poly\_exact2},firstnumber=1]

poly_exact2:=proc(P,n)
local i, coef, coef_t:
global deg, Q, psup, pinf, pfull:
psup:=0: pinf:=0:
Q:=[];
convert(psup, polynom): convert(pinf, polynom):
deg:=degree(P,x):
  for i from 0 to deg do
    coef:=coeff(P,x,i):
    if (coef=0) then
      Q:= (Q,[0,0]):
    elif(coef=1) then
        Q:= (Q,[1,0]):
        psup:=psup+x^i:
    else        
      coef_t:=hi_lo(coef):
      Q:= (Q,[coef_t]):
      psup:=psup + coef_t[1]*x^i:
        if(i<n) then
        pinf:=pinf + coef_t[2]*x^i:
        fi;
    end if;
  od:
Q:=Q[2..deg+2];
pfull:=expand(psup+pinf):
end:
\end{lstlisting}
\vspace{0.5cm}



The following Maple procedure gives a bound on the accumulated
rounding errors in the evaluation of a polynomial returned by the
previous procedure, assuming all the operations are performed in
IEEE-754 round to the nearest mode, and assuming $x$ is exact.

Here is an explanation of this computation, for a polynomial of degree 4.




%\newcommand{\maxx}{{\mathrm{max}_{|x|}}}
%\newcommand{\maxs}[1]{{\mathrm{max}_{|s_{#1}|}}}

 We note 
\begin{itemize}
\item $P$ the polynomial, $d$ its degree
\item $c_j$ the coefficient of $P$ of degree $j$
\item $S_j(x)$ the intermediate polynomial in the Horner evaluation:
  $S_d=c_d$ and $S_j(x) = c_j+xS_{j+1}$ for $1\le j <d$
\item $\maxx$ the maximum value of $|x|$ over the considered interval
\item $p_j = x \otimes s_j $ 
\item $s_j =   c_j \oplus p_{j+1}$ with $s_d = c_d$ 
\item $\maxs{j}$ the  maximum value that $s_j$ may take for $|x|
  \le \maxx$.
\item $\infnorm{S_j}$ the infinite norm of $S_j$ for $-\maxx \le
  x\le \maxx$.
\end{itemize}

Given $ |x| \leq \maxx$, we have

\begin{align*}
  s_4 & \ = \ c_4\\
  \maxs{4} & \ = \  |c_4|\\
  p_4 & \ = \  x \otimes s_4 \ = \  x c_4 + \epsilon_{-53} x c_4 \\
      & \ = \  xc_4 + \epsilon_4 \quad \mathrm{with} \quad |\epsilon_4| \le \delta_4 = 2^{-53}\maxx \maxs{4}\\
  s_3 & \ = \  c_3 \oplus p_4 \ = \  c_3 + p_4 + \epsilon_{-53}(c_3 + p_4) \\
      & \ = \  c_3 +  xc_4 + \epsilon_4 + \epsilon_{-53}(c_3 + xc_4 + \epsilon_4) \\
      & \ = \  c_3 +  xc_4 + \epsilon'_3  
      \quad \mathrm{with} |\epsilon'_3| \le \delta'_3\\
  \delta'_3 & \ =\  \delta_4 + 2^{-53}(\infnorm{c_3 + xc_4} + \delta_4)\  =\  \delta_4 + 2^{-53}(\infnorm{S_3} + \delta_4)\\
  \maxs{3} & \ = \  \infnorm{c_3 + xc_4} + \delta'_3 \ = \  \infnorm{S_3} + \delta'_3\\ 
  p_3 & \ = \  x \otimes s_3 \ = \  x s_3 + \epsilon_{-53} x s_3 \\
      & \ = \  x(c_3 +  xc_4 + \epsilon'_3) + \epsilon_{-53} x s_3\\
      & \ = \  x(c_3 +  xc_4) + x\epsilon'_3 + \epsilon_{-53} x s_3\\
      & \ = \  x(c_3 +  xc_4) +  \epsilon_3 \quad \mathrm{with} \quad 
      |\epsilon_3| \le \delta_3\\
  \delta_3 & \ =\ \maxx\delta'_3 + 2^{-53} \maxx \maxs{3}\\
...
\end{align*}


