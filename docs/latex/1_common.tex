% Common notations, theorems and procedures

\section{Notations\label{section:notations}}


The following notations will be used throughout this document:
\begin{itemize}

\item  $+$, $-$ and  $\times$ denote the usual
mathematical operations.

\item $\oplus$, $\ominus$ and $\otimes$ denote the
corresponding floating-point operations in IEEE-754 double precision,
in the IEEE-754 \emph{round to nearest} mode.

\item $\epsilon$ (usually with some index) denotes an error and
  $\delta$ (with the same index) denotes a bound on this error.

\item $\epsilon_{-k}$ -- with a negative index -- denotes an error bounded by $2^{-k}$.
  
\item For a floating-point number $x$, the value of the least
  significant bit of its mantissa is classically denoted $\ulp(x)$.

\end{itemize}




\section{Common C procedures  for double-precision numbers\label{section:commonCdouble}}

\subsection{Sterbenz Lemma}

\begin{theorem}[Sterbenz Lemma~\cite{Ste74,Gol91}]
\label{sterbenz}
If $x$ and $y$ are floating-point numbers, and if ${y}/{2} \leq x \leq 2y$ then $x\ominus y$ is computed exactly, without any rounding error.
\end{theorem}


\subsection{Double-precision numbers in memory\label{section:memory}}

A double precision floating-point number uses $64$ bits that is two
times the size of an integer usually represented with $32$ bits. The
order in which are stored in memory the two $32$ bits words depends on
the architecture. An architecture is said \emph{Little Endian} if the
lower part of the number is stored in memory at the smallest address;
x86 processor use this representation. Conversely, an architecture
with the high part of the number stored in memory at the smallest
address is said \emph{Big Endian}; PowerPC processor use this
representation.


The following code extract from a double precision number $x$ its
upper, and lower part.

\begin{lstlisting}[label={chap0:lst:endian},
  caption={Extract upper and lower part of a double precision number $x$},firstnumber=1]
/* LITTLE_ENDIAN/BIG_ENDIAN are defined by the user or  */
/* automatically by tools such as autoconf/automake.   */

#ifdef LITTLE_ENDIAN
#define HI(x) *(1+(int*)&x)
#define LO(x) *(int*)&x
#elif BIG_ENDIAN
#define HI(x) *(int*)&x
#define LO(x) *(1+(int*)&x)
#endif
\end{lstlisting}



\subsection{Conversion from floating-point to integer}

\begin{theorem}[Conversion floating-point to integer \cite{AMD01}]
  The following algorithm convert a floating-point number $d$ into an
  integer $i$ with rounding to nearest mode.

\begin{lstlisting}[label={chap0:lst:conversion2},caption={Solution 2},firstnumber=1]
#define DOUBLE2INT(i, d) \
  {double t=(d+6755399441055744.0); i=LO(t);}
\end{lstlisting}
\end{theorem}
This algorithm adds the constant $2^{52}+2^{51}$ to the floating-point
number to put the integer part of $x$, in the lower part of the
floating-point number.  We use $2^{52}+2^{51}$ and not $2^{52}$,
because the value $2^{51}$ is used to contain possible carry
propagations with negative numbers.



\section{Common C procedures for double-double arithmetic\label{section:commonCdoubledouble}}

In this section, we give the basic algorithms for computing with
numbers represented the sum of two floating-point numbers (or
\emph{double-double} numbers). The algorithms are given as plain C
functions, but it may be preferable, for performance issue, to
implement them as macros or inlined functions. This implementation
decision depends on the ability of the compiler.


\subsection{Exact sum algorithm {Add12}}

This algorithm is also known as the Fast2Sum algorithm in the litterature.
\begin{theorem}[Exact sum~\cite{Knu73, Boldo2001}]
  Let $a$ and $b$ be floating-point numbers, then the following method
  computes two floating-point numbers $s$ and $r$, such that $s+r =
  a+b$ exactly, and $s$ is the floating-point number which is closest
  to $a+b$.

\begin{lstlisting}[label={lst:Add12Cond},caption={Add12Cond},firstnumber=1]
void Add12Cond(double *s, double *r, a, b) 
{
  double z;
  s = a + b;            
  if ((HI(a)&0x7FF00000)> (HI(b)&0x7FF00000)){  
    z = s - a;           
    r = b - z;           
  }else {                 
    z = s - b;           
    r = a - z;           
  } 
}                         
\end{lstlisting}
This algorithm requires $3$ floating-point additions, $2$ masks and $1$ test over integer.
\end{theorem}


If we are able to prove that  the exponent of $a$ is always greater than that
of $b$, then the previous algorithm to perform an exact addition of 2
floating-point numbers becomes :
\begin{lstlisting}[label={lst:Add12},caption={Add12},firstnumber=1]
void Add12Cond(double *s, double *r, a, b) 
{
  double z;
  s = a + b;            
  z = s - a;  
  r = b - z; 
}            
\end{lstlisting}
The cost of this algorithm is $3$ floating-point additions.






\subsection{Exact product algorithm {Mul12}}

This algorithm is also known as the Dekker algorithm.

\begin{theorem}[Exact product \cite{Dek71,Knu73}]
  Let $a$ and $b$ be two floating-point numbers, with $p \geq 2$ the
  size of their mantissa. Let $c=2^{\frac{\lceil p \rceil}{2}}+1$. The
  following method computes the two floating-point numbers $r1$ and
  $r2$ such that $r1 + r2 = a + b$ with $r1 = a \otimes b$ in the case
  $p=53$ (double precision):


\begin{lstlisting}[label={lst:Mul12Cond},caption={Mul12Cond},firstnumber=1]
void Mul12Cond(double *r1, double *r2, double a, double b){
  double two_m53 = 1.1102230246251565404e-16;  /* 0x3CA00000, 0x00000000 */
  double two_53  = 9007199254740992.;          /* 0x43400000, 0x00000000 */
  double c        = 134217729.;                /* 0x41A00000, 0x02000000 */ 
  double u, up, u1, u2, v, vp, v1, v2, r1, r2;

  if (HI(a)>0x7C900000) u = a*two_m53; 
  else            u = a;
  if (HI(b)>0x7C900000) v = b*two_m53; 
  else            v = b;

  up = u*c;        vp = v*c;
  u1 = (u-up)+up;  v1 = (v-vp)+vp;
  u2 = u-u1;       v2 = v-v1;
  
  *r1 = u*v;
  *r2 = (((u1*v1-*r1)+(u1*v2))+(u2*v1))+(u2*v2)

  if (HI(a)>0x7C900000) {*r1 *= two_e53; *r2 *= two_53;} 
  if (HI(b)>0x7C900000) {*r1 *= two_e53; *r2 *= two_53;} 
} 
\end{lstlisting}

We have to test $a$ and $b$ before and after the core of the
algorithms in order to avoid overflow by multiplying by $c$. The
global cost in the worst case is $4$ tests over integers, $10$
floating-point additions and $13$ floating-point multiplications.
\end{theorem}

If we are able to prove that $a$ and $b$ are less then $2^{970}$, we can skip
this test, and get the following algorithm:
\begin{lstlisting}[label={lst:Mul12},caption={Mul12},firstnumber=1]
void inline Dekker(double *r1, double *r2, double u, double v){
  double c        = 134217729.;                /* 0x41A00000, 0x02000000 */ 
  double up, u1, u2, vp, v1, v2;

  up = u*c;        vp = v*c;
  u1 = (u-up)+up;  v1 = (v-vp)+vp;
  u2 = u-u1;       v2 = v-v1;
  
  *r1 = u*v;
  *r2 = (((u1*v1-*r1)+(u1*v2))+(u2*v1))+(u2*v2)
}
\end{lstlisting}
which reduces the cost of this algorithm to $10$ floating-point
additions and $7$ floating-point multiplications.


\subsection{Double-double addition {Add22}}
  
This algorithm computes the sum of two double-double numbers as a double-double.

\begin{lstlisting}[label={Add22},caption={Add22},firstnumber=1]
void Add22( double * z, double * zz, double x, double xx, double y, double yy){
double r,s;

r = x+y;
s = (ABS(x) > ABS(y))? (x-r+y+yy+xx) : (y-r+x+xx+yy);
*z = r+s;
*zz = r - (*z) + s;
}
\end{lstlisting}

Here ABS is a macro that returns the absolute value of a floating-point number.


\subsection{Double-double multiplication {Mul22}}

\begin{lstlisting}[label={Mul22},caption={Mul22},firstnumber=1]
void Mul22(double x, double xx, double y, double yy, double * z, double * zz){
  double c, cc;
  
  Mul12(&c, &cc, x, y);
  cc = x*yy + xx*y + cc;
  *z = c+cc;
  *zz = c - (*z) + cc;
}  
\end{lstlisting}





\section{Test if rounding is possible\label{section:testrounding}}

We assume here that an evaluation of $y=f(x)$ has been computed with a
total relative error smaller than $\delta$, and that the result is
available as the sum of two non-overlapping floating-point numbers
$y_h$ and $y_l$ (as is the case if computed by the previous
algorithms). This section gives and proves algorithms for testing if
$y_h$ is the correctly rounded value of $y$.




\subsection{Rounding to the nearest}

\begin{theorem}[Correct rounding of a double-double to the nearest double, avoiding denormals]
\label{th:roundingRN1}
~\\
      Let $y$ and $\delta$ be real numbers, and $e$, $y_h$ and $y_l$ be
  floating-point numbers such that 
  \begin{itemize}
  \item $y_h=y_h\oplus y_l$,
  \item none of $y_h$ and $y_l$ is a  NaN.
  \item $|y_h|\ge 2^{-1022+53}$ 
  \item $|y_h+y_l - y| < \delta.|y|$
  \item $0< \delta \le 2^{-53-k}$ with $k\ge 2$ integer
  \item $e\ge 1+  \dfrac{2^{53+k+1}\delta}{(2^{k}-1)(1-2^{-53})}$
\end{itemize}

The following test determines whether $y_h$ is the
  correctly rounded value of $y$ in  round to nearest mode.

\begin{lstlisting}[
  firstnumber=1]
if( (*@$y_h$@*) == ((*@$y_h$@*) + ((*@$y_l$@*)*e)) )
  return (*@$y_h$@*);
else /* more accuracy is needed, lauch accurate phase */
\end{lstlisting}
\end{theorem}

\begin{proof}
  The theorem holds if $y_h$ is $\pm \infty$.
  
  It also holds if $y_h$ is a denormal: in this case $y_l$ is zero, so
  the correctly rounded result is $y_h$, and the test is true. In the
  following we therefore assume that $y_h$ is a normal number. We also
  assume that $y\ge0$ (so $y_h\ge0$), as the other case is symmetric.
  
  Let us note $u=\ulp(y_h)$. By definition of the \ulp, we have $y_h
  \in [2^{52}u, (2^{53}-1)u]$, which implies $y<2^{53}u$ as $y < y_h + y_l + \delta y
  < (2^{53}-1)u +\frac{1}{2}u +\frac{1}{2}u$.

  What we want to prove is that if the test is true, then $y_h =
  \round(y)$. We will prove that  if the test is true, then $|y_l-\frac{1}{2}u| > 2^{53}\delta
  u$, which implies $|y_l-\frac{1}{2}u| > \delta y$, which implies
  that we are not in a difficult case for correct rounding to the
  nearest, so $y_h = \round(y)$.

  Consider the case when $y_l$ is positive:  $0\le y_l \le \frac{1}{2}u$.

  If $0 \le y_l < (\frac{1}{2} - \frac{1}{2^{k+1}})u$, then since
    $|\delta|<2^{-53+k}$ we are in an easy case for rounding to the
    nearest, and $y_h = \round(y)$ regardless of the result of the test.

    Now consider the case when $y_l \ge (\frac{1}{2} - \frac{1}{2^{k+1}})u$.
   % = \frac{2^{k}-1}{2^{k+1}}u
    Remark that the condition $|y_h|\ge 2^{-1022+54}$ ensures that
    $\frac{1}{2}u$ is a normal number, so in this case
    $y_l$ is a normal number. As $e>1$ this ensures that
    $$y_l\times e(1-2^{-53})\ \le\ y_l \otimes e\ \le\ y_l\times e(1+2^{-53})$$


  Suppose that the test is true ($y_h \oplus y_l \otimes e = y_h$). 
  With IEEE-54 compliant rounding to
  nearest, this implies $|y_l \otimes e|
  \le \frac{1}{2}u$, which in turn implies $|y_l \times e|
  (1-2^{-53}) \le \frac{1}{2}u$, as $y_l$ is a normal number and $e>1$. 
  
  We get $y_l \times (1+e-1)(1-2^{-53}) \le \frac{1}{2}u$, or
  \begin{equation}
  \frac{1}{2}u - y_l \ge y_l (e-1)(1-2^{-53})\label{eq:prooftestRN2}
  \end{equation}  

(\ref{eq:prooftestRN2}) implies
    $\frac{1}{2}u - y_l \ge \frac{2^{k}-1}{2^{k+1}} (e-1)(1-2^{-53}) >
    2^{53}\delta u$, from $e\ge 1+
    \frac{2^{53+k+1}\delta}{(2^{k}-1)(1-2^{-53})}$.

  The case when $y_l$ is negative is similar.
\end{proof}



\begin{theorem}[Correct rounding of a double-double to the nearest double, general case]
\label{th:roundingRN2}
~\\
  Let $y$ and $\delta$ be real numbers, and $e$, $y_h$ and $y_l$ be
  floating-point numbers such that 
  \begin{itemize}
  \item $y_h=y_h\oplus y_l$,
  \item none of $y_h$ and $y_l$ is a  NaN.
  \item $|y_h+y_l - y| < \delta.|y|$
  \item $0< \delta \le 2^{-53-k}$ with $k\ge 2$ integer
  \item $e\ge 1+  \dfrac{2^{53+k+2}\delta}{2^{k}-1}$
\end{itemize}

The following test determines whether $y_h$ is the
  correctly rounded value of $y$ in  round to nearest mode.

\begin{lstlisting}[firstnumber=1]
if( (*@$y_h$@*) == ((*@$y_h$@*) + ((*@$y_l$@*)*e)) )
  return (*@$y_h$@*);
else /* more accuracy is needed, lauch accurate phase */
\end{lstlisting}
\end{theorem}

\begin{proof}
  The proof is identical to the previous theorem, with the exception
  that we cannot ensure anymore that $y_l$ will not be a denormal.
  Therefore the relative error on $y_l\times e$ may be as high as
  $\frac{1}{2}$, and the condition on $e$ is reinforced accordingly.
\end{proof}


\subsubsection*{Notes}

\begin{itemize}
\item In general we will target values of $\delta$ in the order of
  $2^{-53-10}$ to balance the execution times of the quick and
  accurate phase. In this case, using the first theorem almost divides
  by two the frequency of calls to the accurate phase. 
\item If a fused multiply-and-add is  used for the
  computation of $y_h+y_l\times e$, TODO.
\end{itemize}



\subsection{Rounding up}

\begin{theorem}[Correct rounding of a double-double to a double]
  Let $y$ and $\delta$ be real numbers, and $e$, $y_h$ and $y_l$ be
  floating-point numbers such that 
  \begin{itemize}
  \item $y_h=y_h\oplus y_l$,
  \item none of $y_h$ and $y_l$ is a  NaN.
  \item $|y_h+y_l - y| < \delta.|y|$
  \item $0< \delta \le 2^{-53-k}$ with $k\ge 2$ integer
  \item $e\ge 1+  \dfrac{2^{53+k+1}\delta}{(2^{k}-1)(1-2^{-53})}$
\end{itemize}

The following test determines whether $y_h$ is the
  correctly rounded value of $y$ in  round to nearest mode.

\begin{lstlisting}[label={roundingtonearest},
  caption={Test for correct rounding to nearest},
  firstnumber=1]
if( (*@$y_h$@*) == ((*@$y_h$@*) + ((*@$y_l$@*)*e)) )
  return (*@$y_h$@*);
else /* more accuracy is needed, lauch accurate phase */
\end{lstlisting}
\end{theorem}

\subsection{Rounding down}

TODO







\section{The Software Carry Save library}

The accurate phase of two-step function evaluations uses
multiple-precision representation and operators from the \scslib\ 
library.  The software carry-save internal representation of
multiple-precision numbers was designed specifically for simple and
fast implementations of addition and multiplication in the 100-500 bit
precision range. More details on software carry-save are available in
\cite{DefDin2002,DinDef2003}.

The parameters of \scslib\ are set up so that all the operators offer
a relative error better than $2^{-200}$ (actually 208 to 210 bits).
This is a large overkill for all the functions in \crlibm, as the
worst cases computed by Lefevre never require more than 158 bits of
accuracy. This enables simple proofs for the second steps, assuming
the operators in \scslib\ are correct.

Another feature that makes accuracy proofs simple when using \scslib\ 
is the following: The range of SCS numbers includes the range of IEEE
double-precision numbers, including denormals and exceptional cases.
Conversions between SCS format and IEEE-754 doubles, as well as
arithmetic operations, follow the IEEE rules concerning the
exceptional cases. SCS doesn't ensure correct rounding, but provides
conversions to doubles in the four IEEE-754 rounding modes, wich is
enough for the purpose of \crlibm.

A formal proof of correctness of the \scslib\ operators remains to be
done. Currently there is nothing more than good confidence based on
the simplicity of the code.


\subsection{Machine implementation}
 A MP number is represented in the proposed format as a
\emph{Software Carry Save} (SCS) structure $R$, depicted on
Figure~\ref{defourdinechin:fig:representation} and composed of the following
fields:
\begin{description}
\item[\emph{R.digits[$n_r$]}] A table of $n_r$ digits with $m_r$ bits
  of precision, using either integer or FP machine numbers 
  (depending of the architecture);
\item[\emph{R.index}] An integer storing the index of the first digit
  in the range of representable numbers, as depicted on
  Figure~\ref{fig:representation};
 \item[\emph{R.sign}] A sign information.  
\end{description}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.7\textwidth]{fig_scs/exponent_representation} % image file name
\caption{The proposed format \label{fig:representation}}
\end{center}
\end{figure}
  
In other words, the value  $x$ of a representation $R$  is:
\begin{equation}
\label{defourdinechin:eqn4}
x = R.sign \times \sum_{j=1}^{n_r} R.digits[j] \times 2^{m_r * (R.index - j)}
\end{equation}

In such a \emph{normal} SCS number $R$, the bits from $m_I$ to $m_r$
of the $R.digits$ fields are thus set to zero. They will be exploited
by the algorithms to store temporary \emph{carry} information, and are
therefore called \emph{carry-save} bits. An SCS number where these
bits are non-zero is said to be non-normal.


In the case when FP machine numbers are used, their exponent field is
unused: Using it would make alignment of mantissas, in the addition
algorithm, much more complex. However, a trick allows to exploit their
sign bit, along with the well-defined rounding properties of the
IEEE-754 standard, to increase the precision of each word by one bit,
which explains some values of Table~\ref{defourdinechin:table}. Due to lack of space
we do not detail this trick here.


\subsection{Arithmetic operations\label{defourdinechin:sec:ops}}

In this section we present algorithms used to perform basic arithmetic
operations. For easier comprehension, we adopt a syntax very similar
to the C language syntax.

\subsubsection{Conversion from double to SCS}
 A first method for converting a double precision floating
point number $d$ into an SCS representation is to extract the
exponent $d_{exp}$ from $d$, and then determine the corresponding
$R.index$ as the integer part of
$\frac{d_{exp}}{2^{m_r}}$.

Another method, described below, uses only a variable number of
multiplications by $2^{m_r}$ or $2^{-m_r}$. This method is faster than the previous
one when the exponent of $d$ is close to $0$.

%% \begin{small}
%% \begin{algorithm}[double2SCS]\mbox{}\\[-3ex]
%% %\begin{defourdinechinspecialdescription}{Stimulus:}
%%   \begin{description}
%% \item[Input:] A double precision floating point number $d$
%%     \vspace{-2mm}
%%   \item[Output:] A multi-precision number $R$ \vspace{-2mm}
%%   \item[Method:]
%%   \begin{tabbing}
%%     $R.index=0$;\\ 
%%     {\bf if} $d$ $\in$ \{Nan, $+\infty$, $-\infty$\}
%%     {\bf then} \{$R=d$; {\bf return}; \}\\
%%     {\bf if} $d>0$  
%%     \={\bf then}  \= \{ $R.sign = 1$; \}\\
%%     \> {\bf else} \> \{ $R.sign = -1$; ~~ $d = -d$; \}
%%   \end{tabbing}
%%   \vspace{-2ex}
%%   \begin{tabbing}
%%     {\bf while} $d>2^{m_r}$ \= {\bf do} \= \{ $R.index++$; ~~ $d = d \times 2^{-m_r}$; \}\\  
%%     {\bf while} $d<1$ \> {\bf do} \> \{ $R.index --$; ~~ $d = d \times 2^{m_r}$; \}\\
%%     {\bf for(}$i=1$ {\bf;} $i \leq n_r$ {\bf;} $i++$ {\bf)} \\ 
%%     \ind \{ $R.digits[i] = \lfloor d \rfloor$; ~~ 
%%     $d=(d-R.digits[i]) \times 2^{m_r}$; \}
%%   \end{tabbing}
%% \end{description}
%% %\end{defourdinechinspecialdescription}
%% \end{algorithm}
%% \end{small}
TODO:
A REFAIRE COMPLETEMENT


%----------
% ADDITION
%----------
\subsubsection{Addition and subtraction}

The addition of two SCS numbers of the same sign consists in
aligning, then adding digits of the same order. Thanks to the
carry-save bits, all these additions will be \emph{exact} and
\emph{independent}.  However the result will usually not be a normal
SCS number: the sums will have overflown in the carry-save bits. A
\emph{renormalization} procedure is presented in section \ref{defourdinechin:renorm}
to propagate these carry bits and get again a normal SCS number.
However, the advantage of SCS representation is that many SCS
numbers can be summed before needing to perform this expensive step
(the exact number depends on the value chosen for $m_r$, $m_I$ and
$n_r$). For example, with $m_I=64$ and $m_r=30$ (see
Table \ref{defourdinechin:table}), we can add approximately $8 \times 10^{9}$
normal SCS numbers between two renormalizations. 

The subtraction (addition of two numbers of opposite signs) is very
similar to the addition algorithm, although more depending on the
choice of integer or FP format. It may also classically lead to a
cancellation, which may need an update of the index of the result. Due
to lack of space we do not detail this here.

Although all the digit operations are exact, the addition or
subtraction of two numbers also classically involves a rounding error,
due to aligning the digits of same magnitude.




%---------------- 
% MULTIPLICATION
%----------------
\subsubsection{Multiplication}

The multiplication of two normal SCS numbers involves the operations
depicted on the Figure \ref{defourdinechin:fig:multiplication}: The partial products
are computed (in parallel) and summed in columns. We already showed in
section~\ref{defourdinechin:theformat} that none of these operation overflows as long
as equation~\ref{defourdinechin:eqn_general} is satisfied. Again, the result is not a
normal SCS number, and a renormalization procedure (described below)
has to be applied to empty the carry bits. However, a few additions
may follow a multiplication before this renormalization, which allows
for further optimization of algorithms using SCS arithmetic. For
instance, a polynomial evaluation can be implemented with a
renormalization after one multiplication and one addition.

Here also, a rounding error is involved when two $n_r$-digit numbers
are multiplied if the result is to fit on $n_r$ digits. The actual
implementation tests if the most significant digit ($z_1$ on
Figure~\ref{defourdinechin:fig:multiplication}) is null, in which case the index of
the result is that of $z_2$.

A further optimization is to avoid computing the columns of lower
magnitude, at the expense of a known increase in the rounding error.
The significance of such a tradeoff is application-dependent.


\subsubsection{Renormalization (carry propagation) \label{defourdinechin:renorm}}

Renormalization is a carry propagation from the low
order to high order digits:  Starting with an initially null carry, at
each step, the previous carry is added to the current digit, and this
sum is then split into two parts. The low $m_r$ bits are a digit of
the normalized result, and the upper part is the next carry.

The actual algorithm is a little bit more complex. The initial
non-normal number may not be representable exactly as a normal SCS
number, therefore the index of the normalized result has to be
increased by one or two.  Normalization thus again involves a rounding
error.

To get the higher-order part of a floating-point digit, the classical
method is to add, then subtract the constant
$2^{m_I+m_r}+2^{m_I+m_r-1}$. For integer digits, a simple mask is used.



%-----------------
% CONVERSION BACK
%-----------------
\subsubsection{Conversion from SCS to floating-point}

A few (4 in the worst case) multiplications and additions suffice to
get the FP number closest to a SCS number.  For instance, for $m_I=53$
and $m_r=26$, we need to compute $d = A.sign \times 2^{A.index \times
  m_r} \times ( A.digits[0]+ 2^{-m_r} \times A.digits[1]+ 2^{-2.m_r}
\times A.digits[2]+ 2^{-3.m_r} \times A.digits[3])$. The number
$2^{A.index \times m_r}$ is build using integer masks. The actual
implementation of this formula is slightly less simple, but this
conversion is still very fast.


\subsubsection{Mixed 32- and 64-bit arithmetics}

An improvement implemented in \scslib\ was the combined use of integer 32- and 64-bit
arithmetics as follows: 

\begin{itemize}
\item MP digits are stored as 32-bit numbers where only a few bits are
  reserved for carries. This removes the main problem of the initial
implementation \cite{Dinechin2002b}, namely its memory inefficiency.

\item Addition uses 32-bit arithmetic. 

\item In the MP multiplication, the partial products are products of
  two 32-bit digits, which are 64-bit numbers. The column sums need
  thus to be computed using 64-bit arithmetic. This can be expressed
  in the C language in a non-ANSI-C99, but de-facto standard way, as
  follows: 32-bit numbers have the \texttt{unsigned int} type; 64-bit
  numbers have the \texttt{unsigned long long int} type. When
  multiplying two digits, one is first cast into this 64-bit type.
  
  For UltraSPARC architectures (detected at build time) the
  conversion is to floating-point, but we will not detail this
  peculiarity further.
\end{itemize}


This works well because all modern processors either have 64-bit
integer units, or offer instructions which store the 64-bit product of
two 32-bit integers into two 32-bit registers. The compiler does the
rest well, because it is conceptually simple: casting unsigned 32-bit
into unsigned 64-bit is trivial; 64-bit addition is translated
straightforwardly into one 32-bit \emph{add} followed by one 32-bit
\emph{add-with carry}.




\subsubsection{Implementation considerations}

For portability purposes, the implemention uses ANSI C as defined by
the C99 standard, and tries to use a recent version of \texttt{gcc}.
We could not exhibit a case where a native compiler from the processor
vendor (Intel or Sun) gave significantly better results than
\texttt{gcc}, which is probably a consequence of the simplicity of our
code.

However, when tuning for performance, we observed that the same code
which was efficient on one processor could lead to very poor results
on another.  Usually, this difference can be traced down to the
capabilities of the processor itself. The typical example is the
knowingly poor integer multiplication on UltraSPARC II. Sometimes
however, the processor should be able to perform well, and it is the
processor-specific backend of the compiler which is to blame, which
can be checked by observing the assembly code produced.  A typical
example is the casting of 32-bits digits to 64-bit arithmetic (or to
an FP number in the case of the UltraSPARC) in the multiplication
algorithm. In these cases we tried to change the programming style in
a way that works well on all processors. Sometimes it wasn't possible,
in which case the code contains, along with a generic version, several
processor-specific tricky versions of the problematic operation,
selected at compile time thanks to the GNU \texttt{automake/autoconf}
tools.


More surprisingly, we were disappointed by the higher-level
capabilities of the compilers, especially at unrolling loops. Our code
exhibits many small \texttt{for} loops whose size is known at
compile-time (usually $n$). This is the ideal situation for loop
unrolling, a technique well known and described in most textbooks on
compiler design. Options exist in most compilers to turn on this
optimisation. Unfortunately, leaving loop unrolling to the compiler
gives very poor results, even when compared to the non-unrolled case.
Since unrolling the loops by hand in the C code takes a few minutes,
we did it for the version of the library which we use ($m=30$, $n=8$).
It marginally increases the code sizes for this small $n$, and
sometimes provides a twofold improvement on speed, depending of the
processor. Of course, this is not satisfactory: We don't want to do it
for all values of $n$, nor do we want to study for each processor the
tradeoffs involved as $n$ increase. We expect however future compilers
to handle unrolling better, and we were surprised that no compiler had
a clear edge on the other in this respect. Some argue, however, that
this issue is pointless, as superscalarity, along with register
renaming and branch prediction inside modern processors, sum up to the
equivalent of dynamic unrolling of the code. In our tests, it isn't :
unrolling does bring a speed-up.






\section{Common Maple procedures \label{section:commonMaple}}


\subsection{Conversions}

Procedure \texttt{nearest} provides the closest IEEE-double number
from input value \texttt{u}.

\begin{lstlisting}[caption={nearest},firstnumber=1]

nearest := proc(u)
local arrondi, signe, x, exposant, mantisseinfinie, mantisse;
Digits:=200:
if u = 0 then arrondi := 0
else 
  if (u < 0) then signe := -1; x := -u else signe := 1; x := u fi;
  exposant := floor(evalf(log(x)/log(2.0)));
  mantisseinfinie := x*2^(52-exposant);
  if frac(mantisseinfinie) < 0.5 then mantisse := round(mantisseinfinie)
   else
     mantisse := floor(mantisseinfinie);
     if type(mantisse,odd) then mantisse := mantisse+1 fi;
   fi;
   arrondi := signe*mantisse*2^(exposant-52)
fi;
arrondi;
end:
\end{lstlisting}



Procedure \texttt{IEEEdouble} returns the exponent, the mantissa and the
binary conversion of the closest IEEE-double number of input value \texttt{x}.

\begin{lstlisting}[caption={IEEEdouble},firstnumber=1]
IEEEdouble:=proc(x) local signe, logabsx, exposant, mantisse, mantisseinfinie, resultat; 
 Digits:=200;
if (x=0) then [0,0,0]; 
else 
 if (x<0) then signe:=-1:
 else signe:=1:
 fi:
 exposant := floor(evalf(log(signe*x)/log(2.0)));
 if (exposant>1023) then mantisse:=infinity:
 elif (exposant<-1022) then mantisse:=0:
 else 
  mantisseinfinie := signe*x*2^(52-exposant);
  if frac(mantisseinfinie) <
0.5 then mantisse := round(mantisseinfinie)
   else
     mantisse := floor(mantisseinfinie);
     if type(mantisse,odd) then mantisse := mantisse+1 fi;
   fi;
 mantisse:= signe*mantisse*2**(-52);
 Digits := 53;
 resultat:=convert(mantisse*2.0^(exposant),binary);
 fi;
[exposant, mantisse, resultat];
fi;
end:
\end{lstlisting}

Procedure \texttt{hi\_lo} returns two IEEE-double numbers $x\_hi$ and $x\_lo$ so that $x = x\_hi + x\_lo + \epsilon_{-103}$.

\begin{lstlisting}[caption={hi\_lo},firstnumber=1]

hi_lo:= proc(x)
global x_hi, x_lo, res:
local tamp:
x_hi:= nearest(evalf(x)):
res:=x-x_hi:
if (res = 0) then
  x_lo:=0:
else
  x_lo:=nearest(evalf(res)):
end if;
(x_hi,x_lo);
end:
\end{lstlisting}
\vspace{0.5cm}



Procedure "ieee2Hexa" returns the closest IEEE-double number from x, in hexadecimal.

\begin{lstlisting}[caption={ieee2Hexa},firstnumber=1]

ieee2Hexa:= proc(x)
  local signe, hex1, hex2, ma, manti, expo, expos, bina, bin1, bin2, dec1, dec2;
  if(x=0) then resultat:=["00000000","00000000"];
  elif(x=-0) then resultat:=["80000000","00000000"];
  elif(x=2) then resultat:=["40000000","00000000"];
  elif(x=-2) then resultat:=["C0000000","00000000"];
  else
   ma:=IEEEdouble(x);
   expo:=ma[1]:
   if (ma[2]<0) then 
    manti:=2**64 + 2**63+(-ma[2]-1)*2**52+(expo+1023)*2**52;
   else 
     manti:=2**64 + (ma[2]-1)*2**52+(expo+1023)*2**52;
   fi:
   hex2:=convert(manti, hex); 
   hex2:=convert(hex2, string):  
  
   resultat:=[substring(hex2,2..9), substring(hex2,10..18)];
  end if;
  resultat;
end proc:
\end{lstlisting}
\vspace{0.5cm}

Procedure "Hexa2ieee" returns the decimal IEEE-double number associated with the hexadecimal enter value "hexa".

\begin{lstlisting}[caption={Hexa2ieee},firstnumber=1]

Hexa2ieee:= proc(hexa)
local dec, bin, expobin, expo, mantis, sign, hex1, hex2, hexcat;
global res;

hex1:= op(1, hexa):
hex2:= op(2, hexa):
hexcat:= cat(hex1, hex2);
dec:= convert(hexcat, decimal, hex):

if(dec >= 2**63) then
  dec = dec - 2**63:
  sign:= -1:
else
  sign:= 1:
fi;  
expo:= trunc(dec/(2**52))-1023:
mantis:= 1+frac(dec/(2**52));
res:= evalf(sign*2**(expo)*mantis);
end proc:
\end{lstlisting}


\subsection{Procedures for polynomial approximation}


Procedure \texttt{Poly\_exact2} takes in arguments a polynomial \texttt{P} and a
integer \texttt{n}. It returns a truncated polynomial, of wich coefficients
are exactly IEEE-double numbers. The \texttt{n} first coefficients are written
over 2 IEEE-double numbers.
 

\begin{lstlisting}[caption={poly\_exact2},firstnumber=1]

poly_exact2:=proc(P,n)
local i, coef, coef_t:
global deg, Q, psup, pinf, pfull:
psup:=0: pinf:=0:
Q:=[];
convert(psup, polynom): convert(pinf, polynom):
deg:=degree(P,x):
  for i from 0 to deg do
    coef:=coeff(P,x,i):
    if (coef=0) then
      Q:= (Q,[0,0]):
    elif(coef=1) then
        Q:= (Q,[1,0]):
        psup:=psup+x^i:
    else        
      coef_t:=hi_lo(coef):
      Q:= (Q,[coef_t]):
      psup:=psup + coef_t[1]*x^i:
        if(i<n) then
        pinf:=pinf + coef_t[2]*x^i:
        fi;
    end if;
  od:
Q:=Q[2..deg+2];
pfull:=expand(psup+pinf):
end:
\end{lstlisting}
\vspace{0.5cm}



The following Maple procedure gives a bound on the accumulated
rounding errors in the evaluation of a polynomial returned by the
previous procedure, assuming all the operations are performed in
IEEE-754 round to the nearest mode, and assuming $x$ is exact.

Here is an explanation of this computation, for a polynomial of degree 4.




%\newcommand{\maxx}{{\mathrm{max}_{|x|}}}
%\newcommand{\maxs}[1]{{\mathrm{max}_{|s_{#1}|}}}

 We note 
\begin{itemize}
\item $P$ the polynomial, $d$ its degree
\item $c_j$ the coefficient of $P$ of degree $j$
\item $S_j(x)$ the intermediate polynomial in the Horner evaluation:
  $S_d=c_d$ and $S_j(x) = c_j+xS_{j+1}$ for $1\le j <d$
\item $\maxx$ the maximum value of $|x|$ over the considered interval
\item $p_j = x \otimes s_j $ 
\item $s_j =   c_j \oplus p_{j+1}$ with $s_d = c_d$ 
\item $\maxs{j}$ the  maximum value that $s_j$ may take for $|x|
  \le \maxx$.
\item $\infnorm{S_j}$ the infinite norm of $S_j$ for $-\maxx \le
  x\le \maxx$.
\end{itemize}

Given $ |x| \leq \maxx$, we have

\begin{align*}
  s_4 & \ = \ c_4\\
  \maxs{4} & \ = \  |c_4|\\
  p_4 & \ = \  x \otimes s_4 \ = \  x c_4 + \epsilon_{-53} x c_4 \\
      & \ = \  xc_4 + \epsilon_4 \quad \mathrm{with} \quad |\epsilon_4| \le \delta_4 = 2^{-53}\maxx \maxs{4}\\
  s_3 & \ = \  c_3 \oplus p_4 \ = \  c_3 + p_4 + \epsilon_{-53}(c_3 + p_4) \\
      & \ = \  c_3 +  xc_4 + \epsilon_4 + \epsilon_{-53}(c_3 + xc_4 + \epsilon_4) \\
      & \ = \  c_3 +  xc_4 + \epsilon'_3  
      \quad \mathrm{with} |\epsilon'_3| \le \delta'_3\\
  \delta'_3 & \ =\  \delta_4 + 2^{-53}(\infnorm{c_3 + xc_4} + \delta_4)\  =\  \delta_4 + 2^{-53}(\infnorm{S_3} + \delta_4)\\
  \maxs{3} & \ = \  \infnorm{c_3 + xc_4} + \delta'_3 \ = \  \infnorm{S_3} + \delta'_3\\ 
  p_3 & \ = \  x \otimes s_3 \ = \  x s_3 + \epsilon_{-53} x s_3 \\
      & \ = \  x(c_3 +  xc_4 + \epsilon'_3) + \epsilon_{-53} x s_3\\
      & \ = \  x(c_3 +  xc_4) + x\epsilon'_3 + \epsilon_{-53} x s_3\\
      & \ = \  x(c_3 +  xc_4) +  \epsilon_3 \quad \mathrm{with} \quad 
      |\epsilon_3| \le \delta_3\\
  \delta_3 & \ =\ \maxx\delta'_3 + 2^{-53} \maxx \maxs{3}\\
...
\end{align*}


